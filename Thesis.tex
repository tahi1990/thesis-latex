\documentclass[a4paper, 12pt]{article}
% Allow the usage of graphics (.png, .jpg)
\usepackage[pdftex]{graphicx}
\graphicspath{ {./images/} }
\usepackage{apacite}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{amsmath}

% set line spacing
\usepackage{setspace}
\setlength{\parindent}{0pt}
\setlength{\parskip}{2ex}
\linespread{1.3}

\usepackage{tocloft} % for list of equations
% define list of equations
\newcommand{\listequationsname}{\Large{List of Equations}}
\newlistof{myequations}{equ}{\listequationsname}
\newcommand{\myequations}[1]{
   \addcontentsline{equ}{myequations}{\protect\numberline{\theequation}#1}
}
\setlength{\cftmyequationsnumwidth}{2.3em}
\setlength{\cftmyequationsindent}{1.5em}

% Comment the following line to NOT allow the usage of umlauts
\usepackage[utf8]{inputenc}
%custom margins
\usepackage[]{vmargin}
\setpapersize{A4}	
\setmarginsrb{35mm}{30mm}{30mm}{20mm}{0pt}{0mm}{12pt}{13mm}
% Correct hyphenation in urls
\usepackage{url}
% Support long tables
\usepackage[]{longtable}
%Pretty bibliography in UEF format
\usepackage{natbib}
% verbatim code listings
\usepackage{listings}
%Unobtrusive in document links
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\begin{document}

\pagenumbering{gobble}

% do cover pages
\include{titlepage}

\tableofcontents

\listoftables

\listoffigures

\listofmyequations

\cleardoublepage

\begin{abstract} 
Clustering is an efﬁcient way to group data into different classes on basis of the internal and previously unknown schemes inherent of the data. With the development of the location based positioning devices, more and more moving objects are traced and their trajectories are recorded. Therefore, moving object trajectory clustering undoubtedly becomes the focus of the study in moving object data mining. To provide an overview, we survey and summarize the development and trend of moving object clustering and analyze typical moving object clustering algorithms presented in recent years. In this thesis, we ﬁrstly summarize the characteristics of trajectory. Secondly, the measures which can determine the similarity/dissimilarity between two trajectories are discussed. Thridly, the strategies and implement processes of classical moving object clustering
algorithms are analyzed. Finally, the validation criteria are analyzed for evaluating the performance and efﬁciency of clustering algorithms. 
\end{abstract}

\pagebreak

\pagenumbering{arabic} % arabic(1234) numbering, autoreset to 1

\section{Introduction}
The growing use of GPS receivers and WIFI embedded mobile devices equipped with hardware for storing data enables us to collect a very large amount of data, that has to be analyzed in order to extract any relevant information. The complexity of the extracted data makes it a difficult challenge. Knowing principal routes that people or vehicles follow during the day can provide valuable information for analysis of mobility. For instance the presence of important routes not adequately covered by the public transport service could be highlighted by a set of trajectories, which provide information on how to improve it. Trajectory clustering is an appropriate way of analyzing trajectory data, and has been applied to road network extraction, detecting taxi fraud, etc. Additionally, trajectory clustering is used to gather temporal spatial information in the trajectory data and is widespread used in many application areas, such as motion prediction \citep{chen2010searching} and traffic monitoring \citep{atev2006learning}

Trajectory data is recorded in different formats depending on the type of device, object motion, or even purpose. For example, a trajectory is generated by GPS devices include sequence of points in geographical space, with a combination of coordinates and time stamp. In certain specific circumstances, other object-related properties such as direction, velocity or geographical information are added \citep{ying2011semantic,ying2010mining}. This kind of multidimensional data is prevalent in many fields and applications, for example, to understand migration patterns by studying trajectories of animals, predict meteorology with hurricane data, improve athlete’s performance, etc. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\textwidth]{Trajectories.png}
    \caption{Trajectories with varying lengths}
    \label{fig1}
\end{figure}

Given different types of analysis tasks and moving object data applications, measuring how similar two trajectories are is a common technique for most tasks and applications. Therefore, distances are a fundamental component of those tasks and applications of trajectory analysis, allowing us to determine effectively how close two trajectories are. Unlike other simple data types such as ordinal variables or geometric points where the distance description is straightforward, the distance between the trajectories must be carefully defined. It is because trajectories are basically high-dimensional data attached to both spatial and temporal attributes which need to be considered for distance measurements. As such the literature contains dozens of distance measurements for trajectory data. For example, distance measurements measure the sequence-only distance between trajectories, such as Euclidean distance and Dynamic Time Wrapping Distance (DTW); they measure both spatial and temporal dimensions of two trajectories as well.
In order to extract useful patterns from high-volume trajectory data, different methods, such as clustering and classification, are usually used. Clustering is an unsupervised learning method that combines data in groups (clusters) based on distance \citep{han2011data,xu2005survey}. The aim of trajectory clustering is to categorize trajectories in cluster groups based on their similarities. The trajectories existing in each cluster have similar characteristics of movement or behavior within the same cluster and are different from the trajectories in other clusters \citep{berkhin2006survey,besse2015review,yuan2017review}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{Cluster Methods.png}
    \caption{Clustering Methods}
    \label{fig2}
\end{figure}

In general, two main approaches are used for clustering complex data such as trajectories. First, use trajectory-specific clustering algorithms and second, use generic clustering algorithms that support trajectory-specific distance functions (DFs). One of the most suitable clustering algorithms for trajectories is density-based clustering \citep{kriegel2011density} which can extract clusters with arbitrary shape and is also tolerant against outliers \citep{ester1996density}. Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is one of the most popular methods among this family, which is widely employed in trajectory clustering \citep{zhao2019trajectory,cheng2018density,chen2011clustering,lee2007trajectory}. Measurement of similarity is the central focus of the clustering problem; thus, similarity (inverse distance) should be calculated prior to grouping. Distance definition in spatial trajectories is much more complicated than point data. Trajectories are sequences of points in several dimensions that are not of the same length. Thus, in order to compare two trajectories, a comprehensive approach is needed to fully determine their distance. Depending on the analysis purpose and also the data type, different distance functions are presented. The concept of similarity is domain specific, so different distance functions are defined in order to address different aspects of similarity such as spatial, spatio-temporal, and temporal. Spatial similarity is based on spatial parameters like movement path and its shape whereas spatio-temporal similarity uses the temporal component as well. For instance, to extract the movement patterns in trajectories like detecting the transportation mode, besides the trajectory’s geometry, temporal component should also be considered. Among all defined Distance Functions so far, Euclidean, Fréchet, Hausdorff, DTW, LCSS, EDR, and ERP distances are the basic functions in similarity measurements from which so many other functions are generated \citep{abbaspour2017method,aghabozorgi2015time,wang2013effectiveness}.

\section{Trajectory}
A trajectory is a sequence of time-stamped point records describing the motion history of any kind of moving objects, such as people, vehicles, animals, and natural phenomenon. For example, tracking devices with Global Position System (GPS) create a trajectory by tracking object movement as $Trajectory=(A,B,\cdots,T_{n})$, which is consecutive spatial space sequence of points, and $T_{i}$ indicates a combination of coordinates and timestamps such as $T_i=(x_{i},y_{i},t_{i})$.

Theoretically, a trajectory is considered to be a continuous record, i.e., a continuous function of time mathematically, since the object movement is continuous in nature. In practice, however, the continuous location record for a moving object is usually not available since the positioning technology (e.g., GPS devices, road-side sensors) can only collect the current position of the moving object in a periodic manner. Due to the intrinsic limitations of data acquisition and storage devices such inherently continuous phenomena are acquired and stored (thus, represented) in a discrete way. This subsection starts with approximations of object trajectories. Intuitively, the more data about the whereabouts of a moving object is available, the more accurate its true trajectory can be determined.

\subsection{Model for trajectory clustering}
The increasing use of GPS and Wi-Fi mobile devices equipped with data storage hardware allows for the collection of enormous data, for example, of the relevant data to be used in order to find the best way from A to B, detect abnormal behavior, optimize the flow of traffic in a city, foresee the next location or destination of a city. In fact, this aims to build from the data the different characteristics which characterize the different daily movement of road vehicles. We take clustering methods for trajectories into consideration. Clustering techniques are intended to consolidate similar trajectories into groups which differ. The complexity of the trajectory makes this a difficult task because objects in a given area can move along many different paths. 

Several methods can be used to cluster trajectories. We will concentrate on trajectory clustering based on distance but have examined other specific methodologies. Trajectories are usually treated as multidimensional (2D or 3D in most cases) time series data as in \autoref{fig3}; \cite{gaffney1999trajectory}, \cite{vasquez2004motion}, \cite{hu2006system} successfully apply trajectory clustering methods on video-stream trajectories. The continuous definition of trajectory also applies by \cite{gariel2011trajectory} to re-sample trajectories to achieve equal-length time series. These new trajectories are then analyzed for the main components to obtain and finally cluster the primary components. This approach is used for routes in aircraft. All these methods take both the spatial as well as the temporal aspect of the trajectory into account, not adapting to vehicle trajectories limited to a road network of very irregular times. Our main objective is to detect the main track and traffic flow that can be used to study the various behaviours. 

In this context, the aim of this work is to build a collection of paths, which model car drivers' behaviors in a data-driven manner.
These models are learned from a collection of automotive locations.
We focus here on grouping trajectories with similar paths.
This clustering is based on comparing trajectory objects and requires a new definition of the distance between the studied objects. 

A large amount of work has been done to give new definitions of trajectory distance. Several methods have been used to cluster
data set of trajectories. Clustering methods using Euclidean distance lead to inaccurate results mainly because trajectories have different number of points. Hence, several methods based on warping distance have been defined. Typical examples include the distance measures based on DTW, Edit distance and Longest Common Subsequence (LCSS), which reorganize the time index of trajectories to obtain a perfect match between them. Another approach is to focus on the geometry
of the trajectories, in particular their shape. Shape distances like Hausdorff and Fréchet distances can be adapted to trajectories. In next section, we will study and compare several distances on trajectory.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{Trajectory.png}
    \caption{A GPS trajectory with 7076 points from GeoLife Project dataset of Microsoft Research Asia \citep{qian2017simplifying}.}
    \label{fig3}
\end{figure}

\pagebreak

\section{Trajectory Similarity}
There are many ways to define how close two objects are. A trajectory distance generalization that consideres moving objects and it measures on average how close two objects were during some time period. $d(A,B)$ denotes the distance between two trajectories $A\,and\,B$. The larger the value is, the less similar the two trajectories are. Many functions can be used to qualify this dissimilarity beyond the concept of mathematical distance. Distances can be classified into two categories: those whose compare the shape of trajectory (Shape-based distance) and those take into account the temporal dimension (Warping based distance).

\subsection{Warping based distance}
Euclidean distance is the most commonly adopted distance measures that pair-wisely computes the distance between corresponding points of two trajectories. Besides being relatively straightforward and intuitive, Euclidean distance and its variants have several other advantages. The complexity of computing value is linear; in addition, they are easy to implement, indexable withany access method, and parameter free. Euclidean distance was proposed as a distance measure between time series and once considered as one of the most widely used distance functions since the 1960s \citep{keogh2000scaling,faloutsos1994fast,pfeifer1980three,priestley1980state}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{Euclidean.png}
    \caption{Euclidean distance}
\end{figure}

For two trajectories $A\,and\,B$, the Euclidean distance $d_{Euclidean}(A, B)$ with the same size $n$ is define as follows:
\begin{equation} \label{eq1}
    d_{Euclidean}(A, B) = \frac{\sum_{i=1}^n d(a_{i}, b_{i})}{n}
\end{equation}
\myequations{Euclidean distance}

Where $a_{i}$ and $b_{i}$ are the ith sample point of $A$ and $B$ respectively. The time complexity is $O(n)$. 

Euclidean distance measure is straightforward; however, it requires the comparing trajectories to be the same size, which is not common in the actual situation; otherwise it will fail to decide the match pairs of the trajectories to be compared. The aim of warping distance is to solve this problem. To improve the accuracy of similarity measurement, the dynamic time warping algorithm (DTW), longest common subsequence algorithm (LCSS), EDR and ERP were introduced. These distances are defined the same way, but they use different cost functions. Firstly, these distance measures find all the sample point match pairs among the two compared trajectories $A$ and $B$. A sample point match pair, $pair(a_{i},b_{j})$, is formed by two sample points where $a_{i} \in A$ and $b_{j} \in B$. There are several sample point matching strategies such as minimal Euclidean distance or minimal transformation cost. Then these distance measures accumulate the distance for matched pairs or count the number of match pairs to get the final distance results. Thus, the sample point matching strategy is the key for every discrete sequence-only distance measure. The sample point matching strategies can be divided into the following two types:

\textbf{Complete match}: For two compared trajectories $A$ and $B$, complete match strategy requires that every sample point of $A$ and $B$ should be in a match pair, as shown in \autoref{fig4}(a). Thus, the number of matches is $\max(size(A),size(B)$.

\textbf{Partial match}: For two compared trajectories $A\,and\,B$, partial match strategy does not require every sample point of $A\,and\,B$ should be in a match pair, as shown in \autoref{fig4}(b). Thus, some sample points will not be matched to any sample points.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{Matching Methods.png}
    \caption{Matching Methods \citep{su2020survey}}
    \label{fig4}
\end{figure}

\subsubsection{DTW}
\textit{Dynamic time warping} (DTW) is an algorithm for measuring the distance between two sequences. DTW has existed for over a hundred years. Initially, DTW was introduced to compute the distance of time series \citep{myers1980performance}. In 1980s, DTW was introduced to measure trajectory distance \citep{kruskal1983overview} and has become one of the most popular trajectory distance measure since then. DTW distance is defined in a recursive manner and can be easily applied in dynamic programming. It searches through all possible points’alignment between two trajectories for the one with minimal distance. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{DTW.png}
    \caption{DTW distance}
\end{figure}

Specifically, the DTW $d_{DTW} (A,B)$ between two trajectories $A$ and $B$ with lengths of $n$ and $m$ is defined as:
\begin{equation} \label{eq2}
    d_{DTW} (A,B) = \begin{cases}
                                0, if\:n\:=\:0\:and\:m\:=\:0 \\
                                \infty, if\:n\:=\:0\:or\:m\:=\:0 \\
                                d(Head(A), Head(B)) + min\{d_{DTW}(A, Rest(B)), \\ d_{DTW}(Rest(A), B), d_{DTW}(Rest(A), Rest(B) \} \: otherwise
                            \end{cases}
\end{equation}
\myequations{DTW distance}
The time complexity of DTW is $O(mn)$.

\subsubsection{LCSS}
\textit{Longest common subsequence} (LCSS) and edit distance-based distance measures are the mainly used distance metrics of partial match measures. LCSS is a traditional similarity metric for strings, which is to find the longest subsequence common to two compared strings. The value of LCSS similarity between sequences $S_{1}\,and\,S_{2}$ stands for the size of the longest common subsequence of $S_{1}\,and\,S_{2}$. Trajectory can be treated as a sequence of sample points, so LCSS was used to measure the similarity between trajectories. The value of LCSS similarity between trajectories $A\,and\,B$ stands for the size of the longest common subsequence of $A\,and\,B$. However, it can hardly find any two sample points with the exact same location information. Thus, in measuring the similarity of trajectories $A\,and\,B$, LCSS treats $a_{i}\,(a_{i} \in A)\,and\,b_{j}\,(b_{j} \in B)$ to be the same as long as the distance between $a_{i}\,and\,b_{j}$, which is less than a threshold $\epsilon$. Thus, some sample points of $A\,and\,B$ cannot be in any match pairs. Then LCSS distance simply counts the number of match pairs between $A\,and\,B$. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{LCSS.png}
    \caption{LCSS distance}
\end{figure}

Since the sample points far apart do not contribute to the value of LCSS distance, these sample points do not have match points. In contrast to Euclidean distance, LCSS is robust to noise. LCSS distance between trajectories is defined as:
\begin{equation} \label{eq3}
    d_{LCSS}(A, B) = size(A) + size(B) - 2s_{LCSS}(A,B)
\end{equation}
\myequations{LCSS distance}

\subsubsection{Pros and Cons}
The primary advantage of these distances is that they allow for the comparison of trajectories of varying lengths.

The following are the two main limitations of warping-based distance: 
\begin{itemize}
    \item Warping methods rely on a one-to-one comparison of sequences.
    As a result, it is frequently necessary to select a specific series that will be used as a reference, onto which all other sequences will be mapped.
    The index of two sequences being compared should be well balanced in order to best capture variability, such as detecting if there were accelerations and decelerations during time series measurement.
    As a result, the reference sequence must be carefully chosen. 
    \item The performance of traditional warping-based methods is hampered by the large amount of noise inherent in road traffic data, which is not the case when examining time series. 
\end{itemize}

\subsection{Shape based distance}
These distances try to catch geometric features of the trajectories, in particular, their shape instead of matching the trajectory sample points directly. This means trajectories are clustered independently of their location and of a global change of scale. The Hausdorff distance and Fréchet distance are the most well-known distance.

\subsubsection{Fréchet distance}
The Fréchet distance is a measure of similarity between curves that takes into account the location and ordering of the points along the curves. The Fréchet distance between the two curves is the length of the shortest leash sufficient for both to traverse their separate paths. Let $A\,and\,B$ be two trajectories which can be represented as two continuous functions $f_{a}\,and\,f_{b}$ over time $t$, where the starting time and end time of time period $t$ are denoted by $t.start$ and $t.end$ respectively. The Fréchet distance between $A\,and\,B$ is defined as the infimum over all reparameterizations $f_{a}(t)$ and $f_{b}(t)$. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{Frechet.png}
    \caption{Fréchet distance}
\end{figure}

The Fréchet distance algorithm is shown below:
\begin{equation} \label{eq6}
    d_{Frechet}(A,B) = infmax_{t in [t.start, t.end]} \{d(f_{a}(t), f_{b}(t))\}
\end{equation}
\myequations{Fréchet distance}
Since its value is the longest distance between two trajectories at the same time, a noisy point is always far away from a trajectory, causing Fréchet distance to be very sensitive to noise.

The time complexity of Fréchet distance is $O(mn)$. 

\subsubsection{Hausdorff distance}
The Hausdorff distance is a metric. It measures the distance between two sets of metric spaces. Informally, two sets are close in the Hausdorff distance if every point of either set is close to some point of the other set. The Hausdorff distance is the longest distance you can be forced to travel by an adversary who chooses a point in one of two sets, from where you then must travel to other set. In other words, it is the greatest of all the distances from a point in one set to the closest point in the other set.

The Hausdorff distance algorithm is shown below:
\begin{equation} \label{eq7}
    d_{Hausdorff}(A,B) = max \{ \sup_{a \in A} \inf_{b \in B} d(a, b), \sup_{b \in B} \inf_{a \in A} d(a, b) \}
\end{equation}
\myequations{Hausdorff distance}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{Hausdorff.png}
    \caption{Hausdorff distance}
\end{figure}

The time complexity of Hausdorff distance is $O(n^2)$. 

\subsubsection{Pros and Cons}
Both the Fréchet and Hausdorff distances are metrics, which means that they satisfy triangle inequality.
This is a necessary attribute of the distance utilized in clustering algorithms like dbscan or K-medoid if we want the clustering process to be efficient.
They've been frequently employed in a variety of fields that require shape comparison.
However, they may not be able to compare entire trajectories. For example, the Fréchet and Hausdorff distances both return the maximum distance between two trajectories at specific points.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{hausdorff_frechet_distance.png}
    \caption{Three trajectories distances calculated by Hausdorff and Fréchet distance \citep{besse2015review}}
    \label{fig10}
\end{figure}

As in \autoref{fig10}, we can see that $T^1$ and $T^2$ are the most comparable of the three trajectories, yet they are the furthest apart in Fréchet due to the maximum distance of six at the end of the trajectory.
Meanwhile, the Hausdorff distances for the three trajectories are nearly identical, implying poor precision.
These two approaches are effective in applications involving shape comparisons, such as image comparison. 

\subsection{HC-SIM}
\cite{franti2019averaging} proposed a new shape-based distance, HC-SIM (Hierarchical Cell Similarity), a parameter independent variant of C-SIM measure, and we will compare it with other shape-based distances. HC-SIM are proposed as it is the least affected by changes in sampling rate and performs fairly well under noise and point shifting. 

\pagebreak

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=0.8\textwidth]{csim.png}
    \caption{C-SIM}
\end{figure}

C-SIM algorithm was proposed by \cite{mariescu2017grid} to compute similarity between two trajectories. It first retrieves the cell representation and then calculates the similarity measure using the cells. Intersection algorithm computes the intersection between two sets efficiently in linear time using the hashing technique as below:

\textbf{C-SIM}: Computing Similarity Between Two Trajectories

\textbf{Input}: Trajectories $A$ and $B$

\textbf{Ouput}: Similarity

\begin{itemize}
    \item $C_{1}, C_{1}^d \leftarrow Points-to-Cells(A)$
    \item $C_{2}, C_{2}^d \leftarrow Points-to-Cells(B)$
    \item $cab \leftarrow Intersection(A, B)$
    \item $cab^d \leftarrow Intersection(A, B^d)$
    \item $cba^d \leftarrow Intersection(B, A^d)$
    \item $similarity \leftarrow (cab + cab^d + cba^d)/(|C_{1}| + |C_{2}| - cab)$
\end{itemize}

\textbf{Points-to-Cells}: Finding the set of Cells that approximate a given trajectory. \cite{mariescu2017grid} used a hashing method to keep track of cells already existing in the representation. With 25×25-meter-sized cells, a 100×100 km square results in a grid of size 4,000×4,000 cells. A trajectory consists of pairs of Easting and Northing values that passes through the cells. Gaps can appear in the cell representation when user is traveling faster than the cell length divided by sampling interval, or when user moves and the device fails to update the location or for some other reasons. The cells were generated in the order that the trajectory points were recorded; if two consecutively generated cells are not adjacent, the gap is filled by using linear interpolation with equation:

\begin{equation} \label{eq12}
    y = \frac{y_2 - y_1}{x_2 - x_1}(x - x_1) + y_1
\end{equation}
where $(x_1, y_1)$ and $(x_2, y_2)$ are the easting and northing values of the two cells inside a 100km square.

\textbf{Intersection}: Jacard index was used to measure amount of similarity. It is calculated as the size of the intersection divided by the size of the union of two sets \citep{mariescu2017grid}:

\begin{equation} \label{eq13}
    J(C_A, C_B) = \frac{|C_A \cap C_B|}{|C_A \cup C_B|}
\end{equation}

Each of two trajectories are dilated separately, compute the two intersections with original of the other trajectory as follows:

\begin{equation} \label{eq14}
    S(C_A, C_B) = \frac{|C_A \cap C_B| + |C_A \cap C_B^d| + |C_B \cap C_A^d|}{|C_A| + |C_B| - |C_A \cup C_B|}
\end{equation}

The total time complexity of C-SIM is $O(N_A + N_B + |C_A| + |C_B|)$ where $N_A$ and $N_B$ are the number of points in the two trajectories.

In this thesis, we calculate HC-SIM distance through dissimilarity measure:

\begin{equation} \label{eq15}
    \text{HC-SIM}(A, B) = \frac{1}{L} \sum_{i=1}^{L}\text{C-SIM}(A, B)
\end{equation}

\begin{equation} \label{eq16}
    d_\text{HC-SIM}(A, B) = 1 - \text{HC-SIM}(A, B)
\end{equation}

\section{Clustering}
Clustering is the most common unsupervised learning method in pattern recognition. It is basically the task of grouping objects such that similar objects reside in the same group together. Clustering algorithms have gained a lot of attention among researchers and therefore, a large number of clustering algorithms have evolved. In order to be successful with clustering, first we need to understand the nature of the objects we need to cluster. We need to examine their properties and understand how these objects are inputted to an algorithm. For instance, one may be interested to know whether the data that needs to be clustered is inputted to the algorithm incrementally. This is an interesting problem when data is collected as clusters are evolving. On the other hand, the dataset can be present as a whole prior to the execution of the clustering algorithm. Additionally, the properties of the objects that need to be clustered are very important. These properties can give rise to a number of important questions such as if the objects are vector-based or if the objects are metric based. Moreover, one needs to know if objects can be transformed from one form to another so that further analysis can be done on them. One of the most important aspects of clustering is a way of comparing the objects which is widely known as a distance function. 

In this section, we will explore clustering algorithms and make an attempt to show which clustering algorithm is reasonable for trajectory clustering. We first start off with some prerequisites that many clustering algorithms require and show that these requirements are met by the trajectory distance function. Next, we move onto a traditional hierarchical clustering (HAC) algorithm and we will show how HAC attacks the problem. HAC is simple to understand and it may present us with some insight as to what we should expect from a general clustering algorithm.

\subsection{Methods}
Clustering algorithms are generally designed to be able to target a large class of objects. However, some algorithms have specific assumptions on the type of objects that are to be clustered. The most general assumption is a mathematical definition for the objects so they can be stored in memory or on a storage device using a data structure. Perhaps the second most widely accepted requirement is a distance function so the algorithm would be able to say how similar two objects are relative to another. Clustering algorithms use a vector-based representation of the objects so each object can be represented as a point in a k-dimensional space and typically the Euclidean distance is used as the distance function between the points. Observe that such assumptions can be very strong in real world problems. We know that we have a mathematical representation of our trajectories. We have also discussed the distance function between the trajectories and we also know that the trajectory distance function follows the metric space rules: it is symmetrical, and it follows the triangulation property. HAC (Hierarchical Agglomerative Clustering) method only requires a distance function between the objects.

The choice of the clustering method is restricted by the characteristics of the trajectory object. Indeed, trajectories have different lengths which prevents an easy definition of a mean trajectory object. The k-means method cannot be used on our trajectory set, nor spectral clustering methods. k-medoid can be used but an efficient algorithm, like partitioning around medoids, or dbscan method, require a valid metrics. Indeed, these algorithms are based on nearest neighbor and require the distance used to satisfy the triangular inequality. Most of the studied distances, LCSS, DTW, are not metrics. In this way, dbscan or partitioning around medoids algorithms will not be used. Moreover, dbscan depends on two extra parameters that are hard to estimate in this case. To perform the clustering of the trajectories, we will focus on two methodologies: hierarchical cluster analysis (HCA) and affinity propagation (AP). As a matter of fact, HCA and AP can use distance/similarity which does not satisfy the triangle inequality. We point out that the choice of the clustering method is restricted to the trajectory object we deal with. Actually, trajectories have different lengths. HCA and AP are both methods which only require the distance/similarity matrix, and thus can cluster objects of different lengths. Both these methods will be used to evaluate our distance.

\subsubsection{DBSCAN}
The Densely Clustering models measure how closely points are packed together after given the centroids. Inspired by this idea, Density-based spatial clustering of applications with noise (DBSCAN), which has been widely applied to trajectory clustering is proposed in \citeyear{ester1996density}. The key idea is that for any data vector to belong to a cluster, there must be at least a given number of data vectors within a specified radius. In other words, the density of the neighborhood around the data vector must exceed a given threshold. In general, the shape of the neighborhood depends on the choice of distance function.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{DBSCAN.png}
    \caption{DBSCAN \citep{su2020survey}}
\end{figure}

The DBSCAN algorithm \citep{ester1996density, kriegel2011density} takes two global inputs: the radius $r$ and the minimum number of data vectors $k$. Given these two values, the local density of a data vector is defined as the number of data vectors $k_{i}$ that lie within the radius $r$ from the data vector ${x_{i}}$. If the local density is greater than the minimum number of points $k$, then $x_{i}$ is called a core point. The concepts of density-reachability and density-connectivity are the basis for the notion of cluster. \cite{ester1996density} define these as follows: A data vector $p$ is directly density-reachable from another data vector $q$ if the distance between them is shorter than $r$, and $q$ is a core point. A data vector $p$ is density-reachable from a vector $q$ if they are joined by a sequence of data vectors $x_{1},\dots,x_{n}, x_{1}=q, x_{n}=p$ such that the data vector $x_{i+1}$ is directly density-reachable from the data vector $x_{i}$. A data vector $p$ is density-connected to the vector $q$ if there is an intermediate data vector $o$ from which both $p$ and $q$ are density-reachable. Data vectors that are not core points but are part of a cluster but are called border points. Finally, data vectors that do not belong to a cluster are called noise points.

The original DBSCAN algorithm gained widespread popularity because it was very efficient for large data sets. According to \cite{kotsiantis2004recent}, DBSCAN out-performs hierarchical and partitioning algorithms. However, this seems to depend on the dimensionality of the data: \cite{kriegel2011density} gives the time complexity of DBSCAN as $O(N^2)$ in the worst case, although they also claim that the average runtime time complexity can be brought down to $O(NlogN)$ if the dimensionality of the data is not too high and R*-trees are used for indexing the data. For comparison, the worst case time complexity of $k-means$ is $O(INK)$, where $I$ is the number of iterations, $N$ is the number of data vectors, and $K$ is the number of clusters. Hierarchical agglomerative clustering can be performed in $O(\tau N^2)$ where $\tau$ is a small number that refers to the number of nearest neighbors that need to be updated after merging clusters \citep{franti2006fast}.

DBSCAN does not require the number of clusters to be given as a parameter because of the way the clusters are formed based on the connectivity of data vectors to each other. The algorithm can generally be used for any data set where a distance function between two data vectors can be defined. Like with non-parametric methods in general, an important advantage is that it can discover clusters of varying sizes and shapes. DBSCAN can also handle outliers because of to its concept of noise.

High-dimensional data can also make clustering more challenging distances between data vectors become more uniform, effectively making them seem more similar \citep{ertoz2003finding}. Furthermore, choosing the parameters for the distance and density thresholds requires domain expertise, and the definitions of the algorithm such as density-connectivity and border points are not very simple. A simplified version of the DBSCAN algorithm was proposed by \cite{campello2013density} which gets rid of the concept of border points.

These techniques do not require the number of clusters to be given as an input parameter and also do not make any assumptions about the density within the clusters. The basic idea of using DBSCAN for trajectory clustering is that considering a similarity function (such as Euclidean or DTW), each trajectory existing in a cluster should have at least $min T_{r}$ number of trajectories in its neighbourhood within a radius of $Eps$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{DBSCAN Trajectory Clustering.png}
    \caption{DBSCAN Trajectory Clustering \citep{su2020survey}}
\end{figure}

This is basic idea of using DBSCAN in trajectory clustering task according to \cite{moayedi2019evaluation} with trajectory dataset $TD$:

\begin{itemize}
    \item Eps-neighborhood of trajectory $T_{i} \in TD$ is defined by $N_{Eps(T_{i})} = \{T_{j} \in TD|dis(T_{i}, T_{j}) \leq Eps\}$. If Eps-neighborhood of $T_{o}$ has at least $minTr$ trajectories, $T_{o}$ is called a core trajectory.
    \item $T_{i}$ is directly density-reachable to $T_{o}$ with respect to Eps, $minTr$, and DF if $T_{i} \in N_{Eps}(T_{o})$ and $|N_{Eps}(T_{o})| \geq minTr$ 
    \item For a give Eps, $minTr$ and DF, trajectory $T_{p}$ is density-reachable from a trajectory $T_{q}$ if there exists a series of trajectories $A,\dots,T_{n}, A=T_{q}, T_{n}=T_{p}$ in the condition that $T_{k}$ is directly density-reachable to $T_{k+1}$.
    \item Trajectories $T_{i}$ and $T_{j}$ are density-connected for a given Eps, $minTr$ and DF, if there is a trajectory To such that both $T_{i}$ and $T_{j}$ are density-reachable from it.
    \item A cluster $C$ is a non-empty subset of $TD$, for every $T_{p}, T_{q}$: if $T_{p} \in C, T_{q}$ is density-reachable from $T_{p}$ then $T_{q} \in C$ and $T_{p}$ is density-connected to $T_{q}$. A trajectory which is not part of any clusters is defined as an outlier.
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{DBSCAN disadvantage.png}
    \caption{The distance threshold $r$ defines the neighborhood of a point. \citep{ertoz2003finding}}
    \label{fig13}
\end{figure}

A disadvantage of DBSCAN is that it struggles with data sets that contain clusters of varying densities \citep{ertoz2003finding}. An example of this is shown in \autoref{fig13} If the chosen distance threshold is too small, all the points belonging to the sparse cluster will be considered noise. If the threshold is too large, all points will be in the same cluster. A possible solution might be to run the algorithm multiple times and removing the already found clusters from the data set after each run.

\subsubsection{K-Medoid}
K-medoids algorithm groups the data based on their distance to each other. Medoid is the representative of a cluster that has the maximum sum of similarity to others in the cluster that has the minimum distance between this object and the medoid.

Medoid is the representative of a cluster that has the maximum sum of similarity to others in the cluster [9]. It should be one of the objects in the set, which is different from Median [43]. Median is the value separates the higher half from the lower half in the set, it could be the average of two middle numbers if set size is even number, otherwise it is the only middle one. As is introduced in [37], K-medoids is more robust compared with the K-means algorithm because it reduces the influence from outlier and noise; but the time complexity is $O(k(n-k)^2)$, $k$ is the number of clusters, $n$ is is the number
of objects, so it is high complexity. In [39], the author gives us a method to speed up the K-medoids algorithm, so that the time complexity could be reduced to $O(n^2)$.

There are many versions about K-medoids clustering, such as the algorithm uses Voronoi iteration [38], but the most common one is the Partition Around Medoid algorithm (PAM), the basic idea of PAM clustering is below:

\textbf{Partition Around Medoid}: Get clusters by K-medoids clustering

\textbf{Input}: K: the number of clusters; D: the dataset of n objects

\textbf{Output}: K clusters

\textbf{Algorithm}:

\begin{enumerate}
    \item Randomly select K medoids to initialize K clusters
    \item Assign each object to the cluster that has the minimum distance between the object and medoid
    \item While the cost of the configuration decreases:
    \begin{enumerate}
        \item For each medoid m and each non-medoid object o:
        \item Swap m and o, map each object to the closest medoid, recompute the cost (Sum of the distance of objects to their medoid)
        \item If the total cost of the configuration increased in the previous step, then undo the swap.
    \end{enumerate}
\end{enumerate}


\subsubsection{Affinity Propagation}
Afinity propagation (AP) is a centroid based clustering algorithm similar to k Means or K medoids, which does not require the estimation of the number of clusters before running the algorithm. Affinity propagation finds “exemplars” i.e. members of the input set that are representative of clusters.
Both similarities and preferences are often represented through a single matrix, where the values on the main diagonal represent preferences. Matrix representation is good for dense datasets. Where connections between points are sparse, it is more practical not to store the whole n x n matrix in memory, but instead keep a list of sparse matrix. Behind the scene, ‘exchanging messages between points’ is the same thing as manipulating matrices, and it’s only a matter of perspective and implementation.
The main drawbacks of K-Means and similar algorithms are having to select the number of clusters and choosing the initial set of points. Affinity Propagation, instead, takes as input measures of similarity between pairs of data points, and simultaneously considers all data points as potential exemplars. Real-valued messages are exchanged between data points until a high-quality set of exemplars and corresponding clusters gradually emerges.

As an input, the algorithm requires us to provide two sets of data:
    Similarities between data points, representing how well-suited a point is to be another one’s exemplar. If there’s no similarity between two points, as in they cannot belong to the same cluster, this similarity can be omitted or set to -Infinity depending on implementation.
    Preferences, representing each data point’s suitability to be an exemplar. We may have some a priori information which points could be favored for this role, and so we can represent it through preferences.

The algorithm then runs through a number of iterations, until it converges. Each iteration has two message-passing steps:

    \textbf{Calculating responsibilities}: Responsibility $r(i, k)$ reflects the accumulated evidence for how well-suited point $k$ is to serve as the exemplar for point $i$, taking into account other potential exemplars for point $i$. Responsibility is sent from data point $i$ to candidate exemplar point $k$.

    \textbf{Calculating availabilities}: Availability $a(i, k)$ reflects the accumulated evidence for how appropriate it would be for point $i$ to choose point $k$ as its exemplar, taking into account the support from other points that point $k$ should be an exemplar. Availability is sent from candidate exemplar point $k$ to point $i$.

In order to calculate responsibilities, the algorithm uses original similarities and availabilities calculated in the previous iteration (initially, all availabilities are set to zero). Responsibilities are set to the input similarity between point $i$ and point $k$ as its exemplar, minus the largest of the similarity and availability sum between point $i$ and other candidate exemplars. The logic behind calculating how suitable a point is for an exemplar is that it is favored more if the initial a priori preference was higher, but the responsibility gets lower when there is a similar point that considers itself a good candidate, so there is a ‘competition’ between the two until one is decided in some iteration.
Calculating availabilities, then, uses calculated responsibilities as evidence whether each candidate would make a good exemplar. Availability $a(i, k)$ is set to the self-responsibility $r(k, k)$ plus the sum of the positive responsibilities that candidate exemplar $k$ receives from other points.
Finally, we can have different stopping criteria to terminate the procedure, such as when changes in values fall below some threshold, or the maximum number of iterations is reached. At any point through Affinity Propagation procedure, summing Responsibility (r) and Availability (a) matrices gives us the clustering information we need: for point $i$, the $k$ with maximum $r(i, k) + a(i, k)$ represents point i’s exemplar. Or, if we just need the set of exemplars, we can scan the main diagonal. If $r(i, i) + a(i, i) > 0$, point $i$ is an exemplar.
We’ve seen that with K-Means and similar algorithms, deciding the number of clusters can be tricky. With AP, we don’t have to explicitly specify it, but it may still need some tuning if we obtain either more or less clusters than we may find optimal. Luckily, just by adjusting the preferences we can lower or raise the number of clusters. Setting preferences to a higher value will lead to more clusters, as each point is ‘more certain’ of its suitability to be an exemplar and is therefore harder to ‘beat’ and include it under some other point’s ‘domination’. Conversely, setting lower preferences will result in having less clusters; as if they’re saying “no, no, please, you’re a better exemplar, I’ll join your cluster”. As a general rule, we may set all preferences to the median similarity for a medium to large number of clusters, or to the lowest similarity for a moderate number of clusters. However, a couple of runs with adjusting preferences may be needed to get the result that exactly suits our needs.
\subsubsection{Hierarchical Clustering}
Some clustering algorithms such as k-means clustering \citep{macqueen1967some} require a constant value (k as input) for the number of clusters prior to execution. The main issue with such algorithms is the unknown nature of data. In a real-world problem, one normally does not expect to have a precise knowledge of how the data is spread out across the space in advance. Therefore, it is a very strong assumption to expect the number of clusters in advance. In contrast, hierarchical clustering algorithms [13] try to tackle these issues by introducing a different view of how clusters can be constructed. Such algorithms usually require a measure of dissimilarity among clusters.

As the name suggests, hierarchical clustering algorithms produce a hierarchical representation of the data. Such hierarchical data representations are normally stored in a tree data structure. Each node of the tree is considered to be a cluster. The root of the tree is at the highest level and it contains all the elements. The root of the tree can be viewed as a single cluster. Each internal node of the tree contains children and each child can be interpreted as a single cluster. Assuming that the entire dataset is given in advance, there are two basic strategies:

\begin{itemize}
    \item Agglomerative (bottom-up)
    \item Divisive (top-down)
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{AP Clustering.png}
    \caption{Hierarchical clustering models \citep{bian2019trajectory}}
    \label{fig14}
\end{figure}

The agglomerative approach starts at the bottom of the tree. Each object is inserted to a leaf node representing a single cluster containing a single object. The algorithm proceeds by merging similar object by constructing internal nodes containing several nodes from the previous level. This is done recursively until there is a single node which defines the root of the tree.

The divisive approach works is opposite direction. Unlike the agglomerative approach, the divisive approach starts off by creating the root of the tree. The algorithm continues by dividing the entire dataset into two subsets of similar objects. This approach is also recursive. The recursion continues until the division process reaches a subset with a single node. 

In this section, we focus on HAC (Hierarchical Agglomerative Clustering) as an example of the hierarchical clustering algorithm. The main idea is to give some insight of how such algorithms are designed. This is also beneficial because the reader will have an idea of what these tree data structures may look like. Assuming that we are given a dataset with $n$ objects to cluster. HAC starts by creating a cluster for each object and then it performs $n - 1$ merges. At each step, the two most similar clusters are merged together. These merges form new internal nodes of the tree containing more objects. Notice that when two nodes are being merged together, the algorithm needs to have a dissimilarity measurement between clusters. This means that the distance function is not enough. However, the cluster dissimilarity measure is derived from the provided distance function. HAC clustering may use one of the following ways to compute the merge cost:
\begin{itemize}
    \item Single Linkage
    \item Complete Linkage
    \item Average Linkage
    \item Ward Linkage
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{HAC Clustering.png}
    \caption{HAC clustering \citep{hierarchicalcluster2020}}
\end{figure}

The dissimilarity measurement is a derivation of distance function. Let $G$ and $H$ be two clusters, the single linkage (SL) measurement is the least distance between any two objects, $g \in G$ and $h \in H$:
\begin{equation} \label{eq17}
    d_{SL}(G, H) = \min_{\substack{g \in G \\ h \in H}}d(g,h)
\end{equation}

The complete linkage (CL) is the opposite of single linkage: complete linkage is the maximum distance betweenany two objects, $g \in G$ and $h \in H$:
\begin{equation} \label{eq18}
    d_{CL}(G, H) = \max_{\substack{g \in G \\ h \in H}}d(g,h)
\end{equation}

The avarage linkage (AL) is the average between groups:
\begin{equation} \label{eq19}
    d_{AL}(G, H) = \frac{1}{|G| \times |H|}\sum_{g \in G}\sum_{h \in H}d(g,h)
\end{equation}

Finally, in the ward linkage (WL) for each cluster a error function is defined. This error function is the average distance of each datapoint in a cluster to the center of gravity in the cluster:
\begin{equation} \label{eq20}
    d_{WL}(G, H) = \frac{n_Gn_H}{n_G + n_H}||\vec{m}_G - \vec{m}_H||^2
\end{equation}

The one we choose to use is called Ward Linkage. Unlike the others, instead of measuring the distance directly, it analyzes the variance of clusters. Ward’s is said to be the most suitable method for quantitative variables.

Where $\vec{m}_j$ is the center of cluster $j$, and $n_j$ is the number of points in it.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.55\textwidth]{Ward Linkage.png}
    \caption{Ward Linkage \citep{hierarchicaltutorial2017}}
\end{figure}

\subsection{Quality Criteria}

A clustering algorithm aims to gather objects into homogeneous groups that are far one from another. In this section, we focus on those scores to evaluate clusters when ground truth labels are known or unknown.

\subsubsection{Silhouette Coefficient}

If the ground truth labels are not known, evaluation must be performed using the model itself. The Silhouette Coefficient \citep{rousseeuw1987silhouettes} is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores:

\begin{itemize}
    \item \textbf{a:} The mean distance between a sample and all other points in the same class.
    \item \textbf{b:} The mean distance between a sample and all other points in the \textit{next nearest cluster}.
\end{itemize}

The Silhouette Coefficient s for a single sample is then given as:

\begin{equation} \label{eq21}
    s = \frac{b - a}{max(a,b)}
\end{equation}

Silhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.

\subsubsection{Fowlkes-Mallows scores}

The Fowlkes-Mallows index \citep{fowlkes1983method} can be used when the ground truth class assignments of the samples is known. The Fowlkes-Mallows score FMI is defined as the geometric mean of the pairwise precision and recall:

\begin{equation} \label{eq22}
    FMI = \frac{TP}{\sqrt{(TP + FP)(TP + FN)}}
\end{equation}

Where $TP$ is the number of \textbf{True Positive} (i.e. the number of pair of points that belong to the same clusters in both the true labels and the predicted labels), $FP$ is the number of \textbf{False Positive} (i.e. the number of pair of points that belong to the same clusters in the true labels and not in the predicted labels) and $FN$ is the number of \textbf{False Negative} (i.e the number of pair of points that belongs in the same clusters in the predicted labels and not in the true labels).

Since the index is directly proportional to the number of true positives, a higher index means greater similarity between the two clusterings used to determine the index. One basic way to test the validity of this index is to compare two clusterings that are unrelated to each other. \cite{fowlkes1983method} showed that on using two unrelated clusterings, the value of this index approaches zero as the number of total data points chosen for clustering increase; whereas the value for the Rand index \citep{steinley2004properties} for the same data quickly approaches making Fowlkes–Mallows index a much more accurate representation for unrelated data. This index also performs well if noise is added to an existing dataset and their similarity compared. \cite{fowlkes1983method} showed that the value of the index decreases as the component of the noise increases. The index also showed similarity even when the noisy dataset had a different number of clusters than the clusters of the original dataset. Thus making it a reliable tool for measuring similarity between two clusters. 

The score ranges from 0 to 1. A high value indicates a good similarity between two clusters.

\section{Implementation}

In this section, we evaluate and compare 5 distances LCSS, DTW, Hausdorff, Fréchet and HCSIM. All these distances have been implemented in Python. 

We also used Python for the implementation of the chosen clustering algorithms, the \textit{scipy} library for \textit{hierarchical clustering analysis}.

\subsection{Ground Truth Data}

\subsubsection{Data}

The data we used are GPS data collected from Mopsi. Mopsi is a website that helps users to find where their friends are and what is around them \citep{mariescu2013detecting}. There are trajectories recorded by users. Those trajectories are displayed on the map with detailed information, such as speed, traveled distance and user’s transportation mode (walking, running, cycling, skiing) which is automatically inferred by the method in \cite{waga2012detecting}. Mopsi allows the user to search trajectories in different ways. It also provides recommendations and tools for managing data collection.

Mopsi data has two types: geo-tagged photos and trajectories. Geo-tagged photos contain information about their location and recorded time. The trajectory is a set of GPS point stored at a fixed interval. In this thesis, we use those trajectories as the data source. There were 444 trajectories in Joensuu, Finland extracted from Mopsi data as shown in \autoref{fig17}.

\begin{figure}[bp!]
    \centering
    \includegraphics[width=1\textwidth]{Mopsi Trajectories.png}
    \caption{Mopsi Trajectories}
    \label{fig17}
\end{figure}

\begin{figure}[bp!]
    \centering
    \includegraphics[width=1\textwidth]{Mopsi Clusters.png}
    \caption{Mopsi Clusters}
    \label{fig18}
\end{figure}

\pagebreak

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1\textwidth]{Mopsi 11 Clusters.png}
    \caption{Mopsi 11 Clusters}
    \label{fig19}
\end{figure}

The data structure is in \autoref{table:1}.

\begin{table}[ht!]
    \centering
    \def\arraystretch{3}%
    \begin{tabular}{||l l l l||} 
     \hline
     \textbf{Column} & \textbf{Type} & \textbf{Description} & \textbf{Example} \\ [0.5ex] 
     \hline\hline
     Latitude & Double & Point latitude & 62.926880 (62° 55' 36.7674") \\ 
     Longitude & Double & Point longitude & 23.184691 (23° 11' 4.8876") \\
     Timestamp & String & Point timestamp & 1559983789 seconds \\
     Altitude & Double & Point altitude & -1.0 meter \\ [1ex] 
     \hline
    \end{tabular}
    \caption{Data structure}
    \label{table:1}
\end{table}

\subsubsection{Trajectory Similarity}

To cluster “similar” trajectories together, it is essential to formulate
a similarity measure between two trajectories. The process computed trajectory similarity was illustrated as follows:

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{Process.png}
    \caption{Overview of our process}
    \label{fig20}
\end{figure}

\autoref{fig20} shows our clustering process that consists of two components. We first identified clues among trajectories then uses them to measure to measure pair-wise similarity among trajectories. With a set of trajectories with pair-wise similarity measures, we labeled trajectories to capture similarity relationship among trajectory pairs. After that based on the label we are able to obtain clusters of trajectories that have high similarity.

\subsubsection{Analysis of the distances}

Hierarchical Clustering models help to understand trajectory by multiple features, so this treetype construction is proper to implement. As shown in \autoref{fig14}, two hierarchical types are also known as “bottom-up” and “top-down” approaches.

In Agglomerative frameworks, trajectories are grouped and the similar clusters are merged by searching their common properties. Optimal clusters are obtained by repeating representation computation and clusters merging until meeting the requirements. Different from Agglomerative, Divisive frameworks cluster trajectory data into groups and split them recursively to reach the requirements. And as mentioned before, we chose Ward Linkage method on HAC (Hierarchical Agglomerative Clustering).

Below figures are Mopsi clustering results using 5 distances LCSS, DTW, Hausdorff, Fréchet and HCSIM with Ward Linkage cluter method.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1\textwidth]{lcss_trajectory_clustering.png}
    \caption{LCSS Trajectories Clustering}
    \label{fig21}
\end{figure}

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1\textwidth]{lcss_11_clusters.png}
    \caption{LCSS 11 Clusters}
    \label{fig22}
\end{figure}

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1\textwidth]{dtw_trajectory_clustering.png}
    \caption{DTW Trajectories Clustering}
    \label{fig23}
\end{figure}

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1\textwidth]{dtw_11_clusters.png}
    \caption{DTW 11 Clusters}
    \label{fig24}
\end{figure}

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1\textwidth]{hausdorff_trajectory_clustering.png}
    \caption{Hausdorff Trajectories Clustering}
    \label{fig25}
\end{figure}

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1\textwidth]{hausdorff_11_clusters.png}
    \caption{Hausdorff 11 Clusters}
    \label{fig26}
\end{figure}

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1\textwidth]{frechet_trajectory_clustering.png}
    \caption{Fréchet Trajectories Clustering}
    \label{fig27}
\end{figure}

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1\textwidth]{frechet_11_clusters.png}
    \caption{Fréchet 11 Clusters}
    \label{fig28}
\end{figure}

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1\textwidth]{hcsim_trajectory_clustering.png}
    \caption{HCSIM Trajectories Clustering}
    \label{fig29}
\end{figure}

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1\textwidth]{hcsim_11_clusters.png}
    \caption{HCSIM 11 Clusters}
    \label{fig30}
\end{figure}

\pagebreak

The Warping-based distances, LCSS and DTW, give the poorest results with LCSS being significantly worse than DTW. The shape-based distance Hausdorff and SSPD give better result. These results confirm that shape-based distances are better adapted than warping-based distances.

\subsubsection{Analysis of the number of cluster selection}

We used \textit{Silhouette Score} to decided the number of clusters.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1\textwidth]{distance_compare.png}
    \caption{Silhouette Score depending on cluster}
    \label{fig31}
\end{figure}

\autoref{fig24} illustrated Silhouette Score based on number of clusters using 5 distances. 10 clusters are found with LCSS, 13 with DTW, 9 with Hausdorff and 11 with SSPD, HCSIM. We got the groundtruth data, and the right number of clusters is 11. So SSPD and HCSIM give the best results.

As 11 is the number of clusters, so we used Fowlkes Mallows index to evaluate cluster results from those 5 distances. \autoref{fig30} showed that HCSIM (shaped base distance) give the better cluster results than warping-based distances with Fowlkes–Mallows Score is over 0.85.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1\textwidth]{fowlkes_mallows_score.png}
    \caption{Fowlkes–Mallows Score based on distance}
    \label{fig32}
\end{figure}

\subsection{Unknown Data}

\subsubsection{Data}
The data we used are GPS data from 536 San-Francisco taxis over a 24-day period. These data are public and can be found on \cite{piorkowski2009crawdad}. We extracted data as shown in \autoref{fig26}.

This data is a blend of 2574 trajectories. All have the same pickup location, the Caltrain station, and all have a drop-off location in downtown San-Francisco.

\begin{figure}[bp!]
    \centering
    \includegraphics[width=1\textwidth]{caltrain_trajectory_map.png}
    \caption{Caltrain Trajectories Dataset}
    \label{fig33}
\end{figure}

\subsubsection{Analysis of the distances}

We used Calinski-Harabasz Index \citep{calinski1974dendrite} to evaluate cluster size for all distances.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{calinski harabasz.png}
    \caption{Calinski-Harabasz Index}
    \label{fig34}
\end{figure}

\pagebreak

The Warping-based distances, LCSS and DTW, give the poorest results with LCSS being significantly worse than DTW. And the two shape-based distances Frechet and Haus-dorff give better results.

Below is the visual results for all distances with 15 clusters. Based on Mopsi experiment, HCSIM is expected to be the best. We observe that trajectories are well classified by HCSIM according to their paths in \ref{fig29} and \ref{fig30}. The clusters using HCSIM seem to be consistent. We obtain a partition of traffic based on the taxi drivers’ behavior leaving the Caltrain station in San Francisco.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1\textwidth]{Caltrain Plots.png}
    \caption{Caltrain Clusters}
    \label{fig35}
\end{figure}

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1\textwidth]{Caltrain DTW.png}
    \caption{Caltrain DTW Clusters}
    \label{fig36}
\end{figure}

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1\textwidth]{Caltrain LCSS.png}
    \caption{Caltrain LCSS Clusters}
    \label{fig37}
\end{figure}

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1\textwidth]{Caltrain Hausdorff.png}
    \caption{Caltrain Hausdorff Clusters}
    \label{fig38}
\end{figure}

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1\textwidth]{Caltrain Frechet.png}
    \caption{Caltrain Frechet Clusters}
    \label{fig39}
\end{figure}

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=1\textwidth]{Caltrain HCSIM.png}
    \caption{Caltrain HCSIM Clusters}
    \label{fig40}
\end{figure}

\pagebreak

\section{Conclustions}
Clustering is an efﬁcient way to analyze and ﬁnd the massive, hidden,  unknown and interesting knowledge in large scale dataset, which facilitates the rapid development of data mining technology in recent decades. With the development of location based service, moving object
clustering becomes a burgeoning topic in related ﬁelds as an essential part of data mining technology. Clustering of non Euclidean objects deeply relies on the choice of a proper distance. In this thesis, we presented different distances focusing on different features of such objects. Based on the experiment, shape-based distances especially give better clusters.

\pagebreak

% Two common styles, pick one or find alternate bst.
% \citet{john} is like John (1985); \citep{jane} is like (Jane 1985)
%\bibliographystyle{IEEEtranN} % Like this (Jane 1995)
\bibliographystyle{apacite} % Like this [6] 
\bibliography{thesis}

\end{document}