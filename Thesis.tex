\documentclass[a4paper, 12pt]{article}
% Allow the usage of graphics (.png, .jpg)
\usepackage[pdftex]{graphicx}
\graphicspath{ {./images/} }
\usepackage{apacite}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{amsmath}

% set line spacing
\usepackage{setspace}
\setlength{\parindent}{0pt}
\setlength{\parskip}{2ex}
\linespread{1.3}

\usepackage{tocloft} % for list of equations
% define list of equations
\newcommand{\listequationsname}{\Large{List of Equations}}
\newlistof{myequations}{equ}{\listequationsname}
\newcommand{\myequations}[1]{
   \addcontentsline{equ}{myequations}{\protect\numberline{\theequation}#1}
}
\setlength{\cftmyequationsnumwidth}{2.3em}
\setlength{\cftmyequationsindent}{1.5em}

% Comment the following line to NOT allow the usage of umlauts
\usepackage[utf8]{inputenc}
%custom margins
\usepackage[]{vmargin}
\setpapersize{A4}	
\setmarginsrb{35mm}{30mm}{30mm}{20mm}{0pt}{0mm}{12pt}{13mm}
% Correct hyphenation in urls
\usepackage{url}
% Support long tables
\usepackage[]{longtable}
%Pretty bibliography in UEF format
\usepackage{natbib}
% verbatim code listings
\usepackage{listings}
%Unobtrusive in document links
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\begin{document}

\pagenumbering{gobble}

% do cover pages
\include{titlepage}

\tableofcontents

\listoftables

\listoffigures

\listofmyequations

\cleardoublepage

\begin{abstract} 
Clustering is an efﬁcient way to group data into different classes on basis of the internal and previously unknown schemes inherent of the data. With the development of the location based positioning devices, more and more moving objects are traced and their trajectories are recorded. Therefore, moving object trajectory clustering undoubtedly becomes the focus of the study in moving object data mining. To provide an overview, we survey and summarize the development and trend of moving object clustering and analyze typical moving object clustering algorithms presented in recent years. In this thesis, we ﬁrstly summarize the characteristics of trajectory. Secondly, the measures which can determine the similarity/dissimilarity between two trajectories are discussed. Thridly, the strategies and implement processes of classical moving object clustering
algorithms are analyzed. Finally, the validation criteria are analyzed for evaluating the performance and efﬁciency of clustering algorithms. 
\end{abstract}

\pagebreak

\pagenumbering{arabic} % arabic(1234) numbering, autoreset to 1

\section{Introduction}
The growing use of GPS receivers and WIFI embedded mobile devices equipped with hardware for storing data enables us to collect a very large amount of data, that has to be analyzed in order to extract any relevant information. The complexity of the extracted data makes it a difficult challenge. Trajectory clustering is an appropriate way of analyzing trajectory data, and has been applied to pattern recognition, data analysis, machine learning, etc. Additionally, trajectory clustering is used to gather temporal spatial information in the trajectory data and is widespread used in many application areas, such as motion prediction \citep{chen2010searching} and traffic monitoring \citep{atev2006learning}

Trajectory data is recorded in different formats depending on the type of device, object motion, or even purpose. In certain specific circumstances, other object-related properties such as direction, velocity or geographical information are added \citep{ying2011semantic,ying2010mining}. This kind of multidimensional data is prevalent in many fields and applications, for example, to understand migration patterns by studying trajectories of animals, predict meteorology with hurricane data, improve athlete’s performance, etc. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\textwidth]{Trajectories.png}
    \caption{Trajectories with varying lengths}
    \label{fig1}
\end{figure}

Given different types of analysis tasks and moving object data applications, calculating the distance between moving object trajectories is a common technique for most tasks and applications. Therefore, distances are a fundamental component of those tasks and applications of trajectory analysis, allowing us to determine effectively how close two trajectories are. Unlike other simple data types, however, such as ordinal variables or geometric points where the distance description is straightforward, the distance between the trajectories must be carefully defined to represent the true underlying distance. It is because trajectories are basically high-dimensional data attached to both spatial and temporal attributes which need to be considered for distance measurements. As such the literature contains dozens of distance measurements for trajectory data. For example, distance measurements measure the sequence-only distance between trajectories, such as Euclidean distance and Dynamic Time Wrapping Distance (DTW); there are trajectory distance measurements measure both spatial and temporal dimensions of two trajectories as well.
In order to extract useful patterns from high-volume trajectory data, different methods, such as clustering and classification, are usually used. Clustering is an unsupervised learning method that combines data in groups (clusters) based on distance \citep{han2011data,xu2005survey}. The aim of trajectory clustering is to categorize trajectory datasets in cluster groups based on their movement characteristics. The trajectories existing in each cluster have similar characteristics of movement or behavior within the same cluster and are different from the trajectories in other clusters \citep{berkhin2006survey,besse2015review,yuan2017review}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{Cluster Methods.png}
    \caption{Clustering Methods}
    \label{fig2}
\end{figure}

In general, two main approaches can be used for clustering complex data such as trajectories. First, identify trajectory-specific clustering algorithms and second, use generic clustering algorithms that use trajectory-specific distance functions (DFs). One of the most suitable clustering methods for trajectories is density-based clustering \citep{kriegel2011density} which can extract clusters with arbitrary shape and is also tolerant against outliers \citep{ester1996density}. Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is one of the most popular methods among this family, which is widely employed in trajectory clustering \citep{zhao2019trajectory,cheng2018density,chen2011clustering,lee2007trajectory}. Measurement of similarity is the central focus of the clustering problem; thus, similarity (inverse distance) should be calculated prior to grouping. Distance definition in spatial trajectories is much more complicated than point data. Trajectories are sequences of points in several dimensions that are not of the same length. Thus, in order to compare two trajectories, a comprehensive approach is needed to fully determine their distance. Depending on the analysis purpose and also the data type, different DFs are presented. The concept of similarity is domain specific, so different DFs is defined in order to address different aspects of similarity such as spatial, spatio-temporal, and temporal. Spatial similarity is based on spatial parameters like movement path and its shape whereas spatio-temporal similarity is based on movement characteristics like speed, and temporal similarity is based on time intervals and movement duration. For instance, in order to extract the movement patterns in trajectories like detecting the transportation mode, besides the trajectory’s geometry, their movement parameters should also be considered. Among all defined Distance Functions so far, Euclidean, Fréchet, Hausdorff, DTW, LCSS, EDR, and ERP distances are the basic functions in similarity measurements from which so many other functions are generated \citep{abbaspour2017method,aghabozorgi2015time,wang2013effectiveness}.

\section{Trajectory}
A trajectory is a sequence of time-stamped point records describing the motion history of any kind of moving objects, such as people, vehicles, animals, and natural phenomenon. For example, tracking devices with Global Position System (GPS) create a trajectory by tracking object movement as $Trajectory=(T_{1},T_{2},\cdots,T_{n})$, which is consecutive spatial space sequence of points, and $T_{i}$ indicates a combination of coordinates and timestamps such as $T_i=(x_{i},y_{i},t_{i})$.

Theoretically, a trajectory should be a continuous record, i.e., a continuous function of time mathematically, since the object movement is continuous in nature. In practice, however, the continuous location record for a moving object is usually not available since the positioning technology (e.g., GPS devices, road-side sensors) can only collect the current position of the moving object in a periodic manner. Due to the intrinsic limitations of data acquisition and storage devices such inherently continuous phenomena are acquired and stored (thus, represented) in a discrete way. This subsection starts with approximations of object trajectories. Intuitively, the more data about the whereabouts of a moving object is available, the more accurate its true trajectory can be determined.

\subsection{Characteristics}
Trajectories are usually treated as multidimensional (2D or 3D in most cases) time series data as in \autoref{fig3}; hence existing distance measures for 1D time series (e.g., stock market data) can be applied directly or with minor extension. Typical examples include the distance measures based on DTW, Edit distance and Longest Common Subsequence (LCSS), which were originally designed for traditional time series but now have been extensively adopted for trajectories. However, with the more widely applicability and deeper understanding of trajectory data, it turns out that trajectories are not simply multidimensional extensions of time series but have some unique characteristics to be taken into account during the design of effective distance measures. We summarize them below.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{Trajectory.png}
    \caption{Trajectory with GPS points}
    \label{fig3}
\end{figure}

\textbf{Asynchronous observations}. Time series databases usually have a central and synchronized mechanism, by which all the data points can be observed and reported to the central repository simultaneously in a controlled manner. For example, in the stock market, the data of all stocks, such as trade price and amount, are reported every 5 seconds simultaneously. In this way, the data points of the stock time series are synchronized, which makes the comparison of two stock data relatively simple. It just needs to compare the pairs of values reported at the same time instant. However, in trajectory databases there is usually no such mechanism to control the timing of collecting location data. Moving objects, such as GPS embedded vehicles, may have different strategies when they need to report their locations to a central repository, such as time-based, distance-based and prediction-based strategies. Even worse, they might suspend the communication with a central server for a while and resume later. The overall result is that the lengths and timestamps of different trajectories are not the same.

\textbf{Explicit temporal attribute}. Although time series data always have the time attribute attached with each data point, in practice we do not explicitly use this information. In other words, time series are usually treated as sequences without temporal information. The reason for doing this is, as mentioned in the first property, all time series data in a system have the same timestamps; hence explicitly maintaining the time attributes is not necessary. However, in trajectory databases, timestamps cannot be dropped because they are asynchronous amongst different trajectories. To make this point clear, we consider two moving objects which travel through the exact same set of geographical locations but take different time duration. Without looking at the temporal attribute, the two trajectories are identical, despite the fact that they have different time periods.

\textbf{More data quality issues}. Traditional time series databases are expected to contain high-quality data since they usually have stable and quality-guaranteed sources to collect the data. Financial data may be one of the most precise time series data and almost error-free. In environmental monitoring applications, data readings from sensors also have little noise. In contrast, trajectory data are faced with more quality issues, since they are generated by individuals in a complex environment. First, GPS devices have measurement precision limits; in other words, what they report to the server might not be the true location of the moving object, but with a certain deviation. Even worse, a GPS device might report a completely wrong location when it cannot find enough satellites to calculate its coordinate. Second, when a device loses power or the moving object travels to a region without GPS signals, its position cannot be sent to the server, resulting in a period of “missing values” in its trajectory data.

\section{Trajectory Distance}
There are many ways to define how close two objects are far one from another. A trajectory distance measure is a method that evaluates the distance between two trajectories. $d(T_{1},T_{2})$ denotes the distance between two trajectories $T_{1}\,and\,T_{2}$. The larger the value is, the less similar the two trajectories are. Distances can be classified into two categories: those whose compare trajectories as sequences consider the spatial attribute only (Shape-based distance) and those consider both spatial and temporal information (Warping based distance). Spatial information means the sequence order of trajectory. Temporal information is time-related information.

\subsection{Warping based distance}
Euclidean distance was the most used distance, but it cannot obtain better accuracy when the local time shifts or when those trajectories lack the same length. In order to improve the accuracy of similarity measurement, the dynamic time warping algorithm (DTW), longest common subsequence algorithm (LCSS), EDR and ERP. These distances are defined the same way, but they use different cost functions. Firstly, these distance measures find all the sample point match pairs among the two compared trajectories $T_{1}$ and $T_{2}$. A sample point match pair, $pair(p_{i},p_{j})$, is formed by two sample points where $p_{i} \in T_{1}$ and $p_{j} \in T_{2}$. There are several sample point matching strategies such as minimal Euclidean distance or minimal transformation cost. Then these distance measures accumulate the distance for matched pairs or count the number of match pairs to get the final distance results. Thus, the sample point matching strategy is the key for every discrete sequence-only distance measure. The sample point matching strategies can be divided into the following two types:

\textbf{Complete match}: For two compared trajectories $T_{1}$ and $T_{2}$, complete match strategy requires every sample point of $T_{1}$ and $T_{2}$ should be in a match pair, as shown in \autoref{fig4}(a). Thus, the match pair number of complete matches is $\max(size(T_{1}),size(T_{2})$.

\textbf{Partial match}: For two compared trajectories $T_{1}\,and\,T_{2}$, partial match strategy does not require every sample point of $T_{1}\,and\,T_{2}$ should be in a match pair, as shown in \autoref{fig4}(b). Thus, some sample points will not be matched to any sample points.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{Matching Methods.png}
    \caption{Matching Methods \citep{su2020survey}}
    \label{fig4}
\end{figure}

\subsubsection{Euclidean distance}
Euclidean distance is the most commonly adopted distance measures that pair-wisely computes the distance between corresponding points of two trajectories. Besides being relatively straightforward and intuitive, Euclidean distance and its variants have several other advantages. The complexity of evaluating these measures is linear; in addition, they are easy to implement, indexable withany access method, and parameter free. Euclidean distance was proposed as a distance measure between time series and was once considered as one of the most widely used distance functions since the 1960s \citep{keogh2000scaling,faloutsos1994fast,pfeifer1980three,priestley1980state}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{Euclidean.png}
    \caption{Euclidean distance}
\end{figure}

For two trajectories $T_{1}\,and\,T_{2}$, the Euclidean distance $d_{Euclidean}(T_{1}, T_{2})$ with the same size $n$ is define as follow:
\begin{equation} \label{eq1}
    d_{Euclidean}(T_{1}, T_{2}) = \frac{\sum_{i=1}^n d(p_{1,i}, p_{2,i})}{n}
\end{equation}
\myequations{Euclidean distance}
Where $p_{1,i}$ and $p_{2,i}$ are the ith sample point of $T_{1}$ and $T_{2}$ respectively. The time complexity is $O(n)$. Euclidean distance measure is straightforward; however, it requires the comparing trajectories to be the same size, which is not common in the actual situation; otherwise it will fail to decide the match pairs of the trajectories to be compared.

\subsubsection{DTW}
\textit{Dynamic time warping} (DTW) is an algorithm for measuring the distance between two sequences. DTW has existed for over a hundred years. Initially, DTW was introduced to compute the distance of time series \citep{myers1980performance}. In 1980s, DTW was introduced to measure trajectory distance \citep{kruskal1983overview} and has become one of the most popular trajectory distance measure since then. DTW distance is defined in a recursive manner and can be easily applied in dynamic programming. It searches through all possible points’alignment between two trajectories for the one with minimal distance. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{DTW.png}
    \caption{DTW distance}
\end{figure}

Specifically, the DTW $d_{DTW} (T_{1},T_{2})$ between two trajectories $T_{1}$ and $T_{2}$ with lengths of $n$ and $m$ is defined as:
\begin{equation} \label{eq2}
    d_{DTW} (T_{1},T_{2}) = \begin{cases}
                                0, if\:n\:=\:0\:and\:m\:=\:0 \\
                                \infty, if\:n\:=\:0\:or\:m\:=\:0 \\
                                d(Head(T_{1}), Head(T_{2})) + min\{d_{DTW}(T_{1}, Rest(T_{2})), \\ d_{DTW}(Rest(T_{1}), T_{2}), d_{DTW}(Rest(T_{1}), Rest(T_{2}) \} \: otherwise
                            \end{cases}
\end{equation}
\myequations{DTW distance}
The time complexity of DTW is $O(mn)$.

\subsubsection{LCSS}
\textit{Longest common subsequence} (LCSS) and edit distance-based distance measures are the mainly used distance metrics of partial match measures. LCSS is a traditional similarity metric for strings, which is to find the longest subsequence com-mon to two compared strings. The value of LCSS similarity between sequences $S_{1}\,and\,S_{2}$ stands for the size of the longest common subsequence of $S_{1}\,and\,S_{2}$. Trajectory can be treated as a sequence of sample points, so LCSS was used to measure the similarity between trajectories. The value of LCSS similarity between trajectories $T_{1}\,and\,T_{2}$ stands for the size of the longest common subsequence of $T_{1}\,and\,T_{2}$. However, it can hardly find any two sample points with the exact same location information. Thus, in measuring the similarity of trajectories $T_{1}\,and\,T_{2}$, LCSS treats $p_{i}\,(p_{i} \in T_{1})\,and\,p_{j}\,(p_{j} \in T_{2})$ to be the same as long as the distance between $p_{i}\,and\,p_{j}$, which is less than a threshold $\epsilon$. Thus, some sample points of $T_{1}\,and\,T_{2}$ cannot be in any match pairs. Then LCSS distance simply counts the number of match pairs between $T_{1}\,and\,T_{2}$. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{LCSS.png}
    \caption{LCSS distance}
\end{figure}

Since the sample points far apart do not contribute to the value of LCSS distance, these sample points do not have match points. In contrast to Euclidean distance, LCSS is robust to noise. LCSS distance between trajectories is defined as:
\begin{equation} \label{eq3}
    d_{LCSS}(T_{1}, T_{2}) = size(T_{1}) + size(T_{2}) - 2s_{LCSS}(T_{1},T_{2})
\end{equation}
\myequations{LCSS distance}

\subsubsection{EDR}
\textit{Edit Distance on Real sequence} (EDR) is an ED-based trajectory distance measure. The EDR distance $d_{EDR}(T_{1},T_{2})$ between two trajectories $T_{1}\,and\,T_{2}$ with lengths of $n\,and\,m$ respectively is the number of edits (insertion, deletion, or substitutions) needed to change $T_{1}\,and\,T_{2}$. Similar to LCSS, it can hardly find any two sample points with exactly the same location information. To measure the distance of trajectories $T_{1}\,and\,T_{2}$, EDR treats $p_{i}\,(p_{i} \in T_{1})\,and\,p_{j}\,(p_{j} \in T_{2})$ the same only if the locations of $p_{i}\,and\,p_{j}$ are within a range $\epsilon$. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{EDR.png}
    \caption{EDR distance}
\end{figure}

The distance $d_{EDR}(T_{1},T_{2})$ is defined as:
\begin{equation} \label{eq4}
    d_{EDR}(T_{1}, T_{2}) = \begin{cases}
                                n, if \:m\:=\:0 \\
                                m, if \:n\:=\:0 \\
                                min\:\{ \\ 
                                d_{EDR}(Rest(T_{1}),Rest(T_{2})) + subcost(Head(T_{1}), Head(T_{2})), \\
                                d_{EDR}(Rest(T_{1}), T_{2}) + 1, d_{EDR}(T_{1}, Rest(T_{2})) + 1 \\
                                \}, otherwise
                            \end{cases}
\end{equation}
\myequations{EDR distance}
Similar to LCSS, EDR is robust to noisy trajectory data. The disadvantage of EDR is that the distance value of EDR heavily relies on the parameter $\epsilon$, which is not easy for users to adjust; a not well adjusted $\epsilon$ may cause inaccuracy. 

The time complexity of EDR is $O(mn)$.

\subsubsection{ERP}
\textit{Edit distance with Real Penalty} (ERP) is a trajectory distance measure that combines Lp-norm and edit distance. As introduced above, Euclidean distance and DTW both use Lp-norm for measuring the distance between trajectories. However, they require every sample point to be in a match pair. ERP uses the edit-distance-like sample point matching method. In edit distance, there are 3 operations, i.e., addition, deletion and substitution. Thus, when a substitution operation happens on a sample point $p_{i}$ from $T_{1}$ for transforming $T_{1}\,to\,T_{2}$, there must be a counterpart $p_{j}$ from $T_{2}$ and ERP treats $p_{i}\,and\,p_{j}$ as a match pair. When an addition operation happens, $p_{j}$ is added to $T_{1}$ for transforming $T_{1}$ to $T_{2}$. ERP treats $p_{j}$ to be matched to an empty point, namely gap. When a deletion operation happens, $p_{i}$ is deleted from $T_{1}$ for transforming $T_{1}\,to\,T_{2}$, and ERP treats $p_{i}$ to be matched to a gap. 
% The distance $d_{ERP}(T_{1},T_{2})$ is defined as:
% \begin{equation} \label{eq5}
%     d_{ERP}(T_{1},T_{2}) = 
% \end{equation}

\subsection{Shape based distance}
These distances try to catch geometric features of the trajectories, in particular, their shape instead of matching the trajectory sample points directly. This means trajectories are clustered independently of their location and of a global change of scale. The Hausdorff distance and Fréchet distance are the most well-known distance.

\subsubsection{Fréchet distance}
The Fréchet distance is a measure of similarity between curves that takes into account the location and ordering of the points along the curves. The Fréchet distance between the two curves is the length of the shortest leash sufficient for both to traverse their separate paths. Let $T_{1}\,and\,T_{2}$ be two trajectories which can be represented as two continuous functions $f_{1}\,and\,f_{2}$ over time $t$, where the starting time and end time of time period $t$ are denoted by $t.start$ and $t.end$ respectively. The Fréchet distance between $T_{1}\,and\,T_{2}$ is defined as the infimum over all reparameterizations $f_{1}(t)$ and $f_{2}(t)$. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{Frechet.png}
    \caption{Fréchet distance}
\end{figure}

The Fréchet distance algorithm is shown below:
\begin{equation} \label{eq6}
    d_{Frechet}(T_{1},T_{2}) = infmax_{t in [t.start, t.end]} \{d(f_{1}(t), f_{2}(t))\}
\end{equation}
\myequations{Fréchet distance}
Since its value is the longest distance between two trajectories at the same time, a noisy point is always far away from a trajectory, causing Fréchet distance to be very sensitive to noise.

The time complexity of Fréchet distance is $O(mn)$. 

\subsubsection{Hausdorff distance}
The Hausdorff distance is a metric. It measures the distance between two sets of metric spaces. Informally, two sets are close in the Hausdorff distance if every point of either set is close to some point of the other set. The Hausdorff distance is the longest distance you can be forced to travel by an adversary who chooses a point in one of two sets, from where you then must travel to other set. In other words, it is the greatest of all the distances from a point in one set to the closest point in the other set.

The Hausdorff distance algorithm is shown below:
\begin{equation} \label{eq7}
    d_{Hausdorff}(T_{1},T_{2}) = max \{ \sup_{x \in X} \inf_{y \in Y} d(x, y), \sup_{y \in Y} \inf_{x \in X} d(x, y) \}
\end{equation}
\myequations{Hausdorff distance}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{Hausdorff.png}
    \caption{Hausdorff distance}
\end{figure}

The time complexity of Fréchet distance is $O(mn)$. 

\subsubsection{Symmetrized Segment-Path Distance (SSPD)}
This distance is a shaped based distance. The Segment-Path distance from trajectory $T_{1}$ to $T_{2}$ is the mean of all distances from points composing $T_{1}$ to trajectory $T_{2}$.
\begin{equation} \label{eq8}
    d_{SPD}(T_{1},T_{2}) = \frac{1}{n} \sum_{i_{1}=1}^{n_{1}}D_{pt}(p_{i_{1}}^1, T_{2})
\end{equation}
Where
\begin{equation} \label{eq9}
    D_{pt}(p_{i_{1}}^1, T_{2}) = min_{i_{2} \in [0,\dots,n_{2}-1]}D_{ps}(p_{i_{1}}^1, s_{i_{2}}^2)
\end{equation}
And $D_{ps}(p_{i_{1}}^1, s_{i_{2}}^2)$ is \textit{Point-to-Segment distance}:
\begin{equation} \label{eq10}
    D_{ps}(p_{i_{1}}^1, s_{i_{2}}^2) = \begin{cases}
                                            n \\
                                            min, otherwise 
                                        \end{cases}
\end{equation}
The distance between $x$ and a segment $s$ is the shortest distance between x and any points of the segment.

The \textit{Symmetrized Segment-Path Distance} is defined as:
\begin{equation} \label{eq11}
    D_{SSPD}(T_{1}, T_{2}) = \frac{D_{SPD}(T_{1}, T_{2}) + D_{SPD}(T_{2}, T_{1})}{2}
\end{equation}

\subsubsection{C-SIM}
C-SIM algorithm was proposed by \cite{mariescu2017grid} to compute similarity between two trajectories. It first retrieves the cell representation and then calculates the similarity measure using the cells. Intersection algorithm computes the intersection between two sets efficiently in linear time using the hashing technique as below:

\textbf{C-SIM}: Computing Similarity Between Two Trajectories

\textbf{Input}: Trajectories $T_{1}$ and $T_{2}$

\textbf{Ouput}: Similarity

\begin{itemize}
    \item $C_{1}, C_{1}^d \leftarrow Points-to-Cells(T_{1})$
    \item $C_{2}, C_{2}^d \leftarrow Points-to-Cells(T_{2})$
    \item $cab \leftarrow Intersection(T_{1}, T_{2})$
    \item $cab^d \leftarrow Intersection(T_{1}, T_{2}^d)$
    \item $cba^d \leftarrow Intersection(T_{2}, T_{1}^d)$
    \item $similarity \leftarrow (cab + cab^d + cba^d)/(|C_{1}| + |C_{2}| - cab)$
\end{itemize}

\textbf{Points-to-Cells}: Finding the set of Cells that approximate a given trajectory. \cite{mariescu2017grid} used a hashing method to keep track of cells already existing in the representation. With 25×25-meter-sized cells, a 100×100 km square results in a grid of size 4,000×4,000 cells. A trajectory consists of pairs of Easting and Northing values that passes through the cells. Gaps can appear in the cell representation when user is traveling faster than the cell length divided by sampling interval, or when user moves and the device fails to update the location or for some other reasons. The cells were generated in the order that the trajectory points were recorded; if two consecutively generated cells are not adjacent, the gap is filled by using linear interpolation with equation:

\begin{equation} \label{eq12}
    y = \frac{y_2 - y_1}{x_2 - x_1}(x - x_1) + y_1
\end{equation}
where $(x_1, y_1)$ and $(x_2, y_2)$ are the easting and northing values of the two cells inside a 100km square.

\textbf{Intersection}: Jacard index was used to measure amount of similarity. It is calculated as the size of the intersection divided by the size of the union of two sets \citep{mariescu2017grid}:

\begin{equation} \label{eq13}
    J(C_A, C_B) = \frac{|C_A \cap C_B|}{|C_A \cup C_B|}
\end{equation}

Each of two trajectories are dilated separately, compute the two intersections with original of the other trajectory as follows:

\begin{equation} \label{eq14}
    S(C_A, C_B) = \frac{|C_A \cap C_B| + |C_A \cap C_B^d| + |C_B \cap C_A^d|}{|C_A| + |C_B| - |C_A \cup C_B|}
\end{equation}

The total time complexity of C-SIM is $O(N_A + N_B + |C_A| + |C_B|)$ where $N_A and N_B$ are the number of points in the two trajectories

\section{Clustering}
Clustering is the most common unsupervised learning method in pattern recognition. It is basically the task of grouping or “clustering” a set of objects such that similar objects reside in the same group together. Clustering algorithms have gained a lot of attention among researchers and therefore, a large number of clustering algorithms have evolved. In order to be successful with clustering a set of objects, first we need to understand the nature of the objects we need to cluster. We need to examine their properties and understand how these objects are inputted to an algorithm. For instance, one may be interested to know whether the data that needs to be clustered is inputted to the algorithm incrementally. This is an interesting problem when data is collected as clusters are evolving. On the other hand, the dataset can be present as a whole prior to the execution of the clustering algorithm. Additionally, the properties of the objects that need to be clustered are very important. These properties can give rise to a number of important questions such as if the objects are vector-based or if the objects are metric based. Moreover, one needs to know if objects can be transformed from one form to another so that further analysis can be done on them. One of the most important aspects of clustering is a way of comparing the objects which is widely known as a distance function. 

In this section, we will explore clustering algorithms and make an attempt to show which clustering algorithm is reasonable for trajectory clustering. We first start off with some prerequisites that many clustering algorithms require and show that these requirements are met by the trajectory distance function. Next, we move onto a traditional hierarchical clustering (HAC) algorithm and we will show how HAC attacks the problem. HAC is simple to understand and it may present us with some insight as to what we should expect from a general clustering algorithm.

\subsection{Methods}
Clustering algorithms are generally designed to be able to target a large class of objects. However, some algorithms have specific assumptions on the type of objects that are to be clustered. The most general assumption is a mathematical definition for the objects so they can be stored in memory or on a storage device using a data structure. Perhaps the second most widely accepted requirement is a distance function so the algorithm would be able to compute the distance between the objects of interest. Some clustering algorithms require a vector-based representation of the objects so each object can be represented as a point in a k-dimensional space and normally the Euclidean distance between these points is used as the distance function between the objects. Observe that such assumptions can be very strong in real world problems. We know that we have a mathematical representation of our trajectories. We have also discussed the distance function between the trajectories and we also know that the trajectory distance function follows the metric space rules: it is symmetrical, and it follows the triangulation property. HAC (Hierarchical Agglomerative Clustering) algorithm only requires a distance function between the objects.

The choice of the clustering method is restricted by the characteristics of the trajectory object. Indeed, trajectories have different lengths which prevents an easy definition of a mean trajectory object. The k-means method cannot be used on our trajectory set, nor spectral clustering methods. k-medoid can be used but an efficient algorithm, like partitioning around medoids, or dbscan method, require a valid metrics. Indeed, these algorithms are based on nearest neighbor and require the distance used to satisfy the triangular inequality. Most of the studied distances, SSPD, LCSS, DTW, are not metrics. In this way, dbscan or partitioning around medoids algorithms will not be used. Moreover, dbscan depends on two extra parameters that are hard to estimate in this case. To perform the clustering of the trajectories, we will focus on two methodologies: hierarchical cluster analysis (HCA) and affinity propagation (AP). As a matter of fact, HCA and AP can use distance/similarity which does not satisfy the triangle inequality. We point out that the choice of the clustering method is restricted to the trajectory object we deal with. Actually, trajectories have different lengths. HCA and AP are both methods which only require the distance/similarity matrix, and thus can cluster objects of different lengths. Both these methods will be used to evaluate our distance.

\subsubsection{DBSCAN}
The Densely Clustering models measure how closely points are packed together after given the centroids. Inspired by this idea, Density-based spatial clustering of applications with noise (DBSCAN), which has been widely applied to trajectory clustering is proposed in \citeyear{ester1996density}. The key idea is that for any data vector to belong to a cluster, there must be at least a given number of data vectors within a specified radius. In other words, the density of the neighborhood around the data vector must exceed a given threshold. In general, the shape of the neighborhood depends on the choice of distance function.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{DBSCAN.png}
    \caption{DBSCAN \citep{su2020survey}}
\end{figure}

The DBSCAN algorithm \citep{ester1996density, kriegel2011density} takes two global inputs: the radius $r$ and the minimum number of data vectors $k$. Given these two values, the local density of a data vector is defined as the number of data vectors $k_{i}$ that lie within the radius $r$ from the data vector ${x_{i}}$. If the local density is greater than the minimum number of points $k$, then $x_{i}$ is called a core point. The concepts of density-reachability and density-connectivity are the basis for the notion of cluster. \cite{ester1996density} define these as follows: A data vector $p$ is directly density-reachable from another data vector $q$ if the distance between them is shorter than $r$, and $q$ is a core point. A data vector $p$ is density-reachable from a vector $q$ if they are joined by a sequence of data vectors $x_{1},\dots,x_{n}, x_{1}=q, x_{n}=p$ such that the data vector $x_{i+1}$ is directly density-reachable from the data vector $x_{i}$. A data vector $p$ is density-connected to the vector $q$ if there is an intermediate data vector $o$ from which both $p$ and $q$ are density-reachable. Data vectors that are not core points but are part of a cluster but are called border points. Finally, data vectors that do not belong to a cluster are called noise points.

The original DBSCAN algorithm gained widespread popularity becauseit was very efficient for large data sets. According to \cite{kotsiantis2004recent}, DBSCAN out-performs hierarchical and partitioning algorithms. However, this seems to depend on the dimensionality of the data: \cite{kriegel2011density} gives the time complexity of DBSCAN as $O(N^2)$ in the worst case, although they also claim that the average runtime time complexity can be brought down to $O(NlogN)$ if the dimensionality of the data is not too high and R*-trees are used for indexing the data. For comparison, the worst case time complexity of $k-means$ is $O(INK)$, where $I$ is the number of iterations, $N$ is the number of data vectors, and $K$ is the number of clusters. Hierarchical agglomerative clustering can be performed in $O(\tau N^2)$ where $\tau$ is a small number that refers to the number of nearest neighbors that need to be updated after merging clusters \citep{franti2006fast}.

DBSCAN does not require the number of clusters to be given as a parameter because of the way the clusters are formed based on the connectivity of data vectors to each other. The algorithm can generally be used for any data set where a distance function between two data vectors can be defined. Like with non-parametric methods in general, an important advantage is that it can discover clusters of varying sizes and shapes. DBSCAN can also handle outliers because of to its concept of noise.

High-dimensional data can also make clustering more challenging distances between data vectors become more uniform, effectively making them seem more similar \citep{ertoz2003finding}. Furthermore, choosing the parameters for the distance and density thresholds requires domain expertise, and the definitions of the algorithm such as density-connectivity and border points are not very simple. A simplified version of the DBSCAN algorithm was proposed by \cite{campello2013density} which gets rid of the concept of border points.

These techniques do not require the number of clusters to be given as an input parameter and also do not make any assumptions about the density within the clusters. The basic idea of using DBSCAN for trajectory clustering is that considering a similarity function (such as Euclidean or DTW), each trajectory existing in a cluster should have at least $min T_{r}$ number of trajectories in its neighbourhood within a radius of $Eps$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{DBSCAN Trajectory Clustering.png}
    \caption{DBSCAN Trajectory Clustering \citep{su2020survey}}
\end{figure}

This is basic idea of using DBSCAN in trajectory clustering task according to \cite{moayedi2019evaluation} with trajectory dataset $TD$:

\begin{itemize}
    \item Eps-neighborhood of trajectory $T_{i} \in TD$ is defined by $N_{Eps(T_{i})} = \{T_{j} \in TD|dis(T_{i}, T_{j}) \leq Eps\}$. If Eps-neighborhood of $T_{o}$ has at least $minTr$ trajectories, $T_{o}$ is called a core trajectory.
    \item $T_{i}$ is directly density-reachable to $T_{o}$ with respect to Eps, $minTr$, and DF if $T_{i} \in N_{Eps}(T_{o})$ and $|N_{Eps}(T_{o})| \geq minTr$ 
    \item For a give Eps, $minTr$ and DF, trajectory $T_{p}$ is density-reachable from a trajectory $T_{q}$ if there exists a series of trajectories $T_{1},\dots,T_{n}, T_{1}=T_{q}, T_{n}=T_{p}$ in the condition that $T_{k}$ is directly density-reachable to $T_{k+1}$.
    \item Trajectories $T_{i}$ and $T_{j}$ are density-connected for a given Eps, $minTr$ and DF, if there is a trajectory To such that both $T_{i}$ and $T_{j}$ are density-reachable from it.
    \item A cluster $C$ is a non-empty subset of $TD$, for every $T_{p}, T_{q}$: if $T_{p} \in C, T_{q}$ is density-reachable from $T_{p}$ then $T_{q} \in C$ and $T_{p}$ is density-connected to $T_{q}$. A trajectory which is not part of any clusters is defined as an outlier.
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{DBSCAN disadvantage.png}
    \caption{The distance threshold $r$ defines the neighborhood of a point. \citep{ertoz2003finding}}
    \label{fig13}
\end{figure}

A disadvantage of DBSCAN is that it struggles with data sets that contain clusters of varying densities \citep{ertoz2003finding}. An example of this is shown in \autoref{fig13} If the chosen distance threshold is too small, all the points belonging to the sparse cluster will be considered noise. If the threshold is too large, all points will be in the same cluster. A possible solution might be to run the algorithm multiple times and removing the already found clusters from the data set after each run.

\subsubsection{K-Medoid}
K-medoids algorithm groups the data based on their distance to each other. Medoid is the representative of a cluster that has the maximum sum of similarity to others in the cluster that has the minimum distance between this object and the medoid.

Medoid is the representative of a cluster that has the maximum sum of similarity to others in the cluster [9]. It should be one of the objects in the set, which is different from Median [43]. Median is the value separates the higher half from the lower half in the set, it could be the average of two middle numbers if set size is even number, otherwise it is the only middle one. As is introduced in [37], K-medoids is more robust compared with the K-means algorithm because it reduces the influence from outlier and noise; but the time complexity is $O(k(n-k)^2)$, $k$ is the number of clusters, $n$ is is the number
of objects, so it is high complexity. In [39], the author gives us a method to speed up the K-medoids algorithm, so that the time complexity could be reduced to $O(n^2)$.

There are many versions about K-medoids clustering, such as the algorithm uses Voronoi iteration [38], but the most common one is the Partition Around Medoid algorithm (PAM), the basic idea of PAM clustering is below:

\textbf{Partition Around Medoid}: Get clusters by K-medoids clustering

\textbf{Input}: K: the number of clusters; D: the dataset of n objects

\textbf{Output}: K clusters

\textbf{Algorithm}:

\begin{enumerate}
    \item Randomly select K medoids to initialize K clusters
    \item Assign each object to the cluster that has the minimum distance between the object and medoid
    \item While the cost of the configuration decreases:
    \begin{enumerate}
        \item For each medoid m and each non-medoid object o:
        \item Swap m and o, map each object to the closest medoid, recompute the cost (Sum of the distance of objects to their medoid)
        \item If the total cost of the configuration increased in the previous step, then undo the swap.
    \end{enumerate}
\end{enumerate}


\subsubsection{Affinity Propagation}
Afinity propagation (AP) is a centroid based clustering algorithm similar to k Means or K medoids, which does not require the estimation of the number of clusters before running the algorithm. Affinity propagation finds “exemplars” i.e. members of the input set that are representative of clusters.
Both similarities and preferences are often represented through a single matrix, where the values on the main diagonal represent preferences. Matrix representation is good for dense datasets. Where connections between points are sparse, it is more practical not to store the whole n x n matrix in memory, but instead keep a list of similarities to connected points. Behind the scene, ‘exchanging messages between points’ is the same thing as manipulating matrices, and it’s only a matter of perspective and implementation.
The main drawbacks of K-Means and similar algorithms are having to select the number of clusters and choosing the initial set of points. Affinity Propagation, instead, takes as input measures of similarity between pairs of data points, and simultaneously considers all data points as potential exemplars. Real-valued messages are exchanged between data points until a high-quality set of exemplars and corresponding clusters gradually emerges.

As an input, the algorithm requires us to provide two sets of data:
    Similarities between data points, representing how well-suited a point is to be another one’s exemplar. If there’s no similarity between two points, as in they cannot belong to the same cluster, this similarity can be omitted or set to -Infinity depending on implementation.
    Preferences, representing each data point’s suitability to be an exemplar. We may have some a priori information which points could be favored for this role, and so we can represent it through preferences.

The algorithm then runs through a number of iterations, until it converges. Each iteration has two message-passing steps:

    \textbf{Calculating responsibilities}: Responsibility $r(i, k)$ reflects the accumulated evidence for how well-suited point $k$ is to serve as the exemplar for point $i$, taking into account other potential exemplars for point $i$. Responsibility is sent from data point $i$ to candidate exemplar point $k$.

    \textbf{Calculating availabilities}: Availability $a(i, k)$ reflects the accumulated evidence for how appropriate it would be for point $i$ to choose point $k$ as its exemplar, taking into account the support from other points that point $k$ should be an exemplar. Availability is sent from candidate exemplar point $k$ to point $i$.

In order to calculate responsibilities, the algorithm uses original similarities and availabilities calculated in the previous iteration (initially, all availabilities are set to zero). Responsibilities are set to the input similarity between point $i$ and point $k$ as its exemplar, minus the largest of the similarity and availability sum between point $i$ and other candidate exemplars. The logic behind calculating how suitable a point is for an exemplar is that it is favored more if the initial a priori preference was higher, but the responsibility gets lower when there is a similar point that considers itself a good candidate, so there is a ‘competition’ between the two until one is decided in some iteration.
Calculating availabilities, then, uses calculated responsibilities as evidence whether each candidate would make a good exemplar. Availability $a(i, k)$ is set to the self-responsibility $r(k, k)$ plus the sum of the positive responsibilities that candidate exemplar $k$ receives from other points.
Finally, we can have different stopping criteria to terminate the procedure, such as when changes in values fall below some threshold, or the maximum number of iterations is reached. At any point through Affinity Propagation procedure, summing Responsibility (r) and Availability (a) matrices gives us the clustering information we need: for point $i$, the $k$ with maximum $r(i, k) + a(i, k)$ represents point i’s exemplar. Or, if we just need the set of exemplars, we can scan the main diagonal. If $r(i, i) + a(i, i) > 0$, point $i$ is an exemplar.
We’ve seen that with K-Means and similar algorithms, deciding the number of clusters can be tricky. With AP, we don’t have to explicitly specify it, but it may still need some tuning if we obtain either more or less clusters than we may find optimal. Luckily, just by adjusting the preferences we can lower or raise the number of clusters. Setting preferences to a higher value will lead to more clusters, as each point is ‘more certain’ of its suitability to be an exemplar and is therefore harder to ‘beat’ and include it under some other point’s ‘domination’. Conversely, setting lower preferences will result in having less clusters; as if they’re saying “no, no, please, you’re a better exemplar, I’ll join your cluster”. As a general rule, we may set all preferences to the median similarity for a medium to large number of clusters, or to the lowest similarity for a moderate number of clusters. However, a couple of runs with adjusting preferences may be needed to get the result that exactly suits our needs.
\subsubsection{Hierarchical Clustering}
Some clustering algorithms such as k-means clustering \citep{macqueen1967some} require a constant value (k as input) for the number of clusters prior to execution. The main issue with such algorithms is the unknown nature of data. In a real-world problem, one normally does not expect to have a precise knowledge of how the data is spread out across the space in advance. Therefore, it is a very strong assumption to expect the number of clusters in advance. In contrast, hierarchical clustering algorithms [13] try to tackle these issues by introducing a different view of how clusters can be constructed. Such algorithms usually require a measure of dissimilarity among clusters.

As the name suggests, hierarchical clustering algorithms produce a hierarchical representation of the data. Such hierarchical data representations are normally stored in a tree data structure. Each node of the tree is considered to be a cluster. The root of the tree is at the highest level and it contains all the elements. The root of the tree can be viewed as a single cluster. Each internal node of the tree contains children and each child can be interpreted as a single cluster. Assuming that the entire dataset is given in advance, there are two basic strategies:

\begin{itemize}
    \item Agglomerative (bottom-up)
    \item Divisive (top-down)
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{AP Clustering.png}
    \caption{Hierarchical clustering models \citep{bian2019trajectory}}
    \label{fig14}
\end{figure}

The agglomerative approach starts at the bottom of the tree. Each object is inserted to a leaf node representing a single cluster containing a single object. The algorithm proceeds by merging similar object by constructing internal nodes containing several nodes from the previous level. This is done recursively until there is a single node which defines the root of the tree.

The divisive approach works is opposite direction. Unlike the agglomerative approach, the divisive approach starts off by creating the root of the tree. The algorithm continues by dividing the entire dataset into two subsets of similar objects. This approach is also recursive. The recursion continues until the division process reaches a subset with a single node. 

In this section, we focus on HAC (Hierarchical Agglomerative Clustering) as an example of the hierarchical clustering algorithm. The main idea is to give some insight of how such algorithms are designed. This is also beneficial because the reader will have an idea of what these tree data structures may look like. Assuming that we are given a dataset with $n$ objects to cluster. HAC starts by creating a cluster for each object and then it performs $n - 1$ merges. At each step, the two most similar clusters are merged together. These merges form new internal nodes of the tree containing more objects. Notice that when two nodes are being merged together, the algorithm needs to have a dissimilarity measurement between clusters. This means that the distance function is not enough. However, the cluster dissimilarity measure is derived from the provided distance function. HAC clustering may use one of the following common flavors of dissimilarity measurements:
\begin{itemize}
    \item Single Linkage
    \item Complete Linkage
    \item Average Linkage
    \item Ward Linkage
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{HAC Clustering.png}
    \caption{HAC clustering \citep{hierarchicaltutorial2017}}
\end{figure}

The dissimilarity measurement is a derivation of distance function. Let $G$ and $H$ be two clusters, the single linkage (SL) measurement is the least distance between any two objects, $g \in G$ and $h \in H$:
\begin{equation} \label{eq15}
    d_{SL}(G, H) = \min_{\substack{g \in G \\ h \in H}}d(g,h)
\end{equation}

The complete linkage (CL) is the opposite of single linkage: complete linkage is the maximum distance betweenany two objects, $g \in G$ and $h \in H$:
\begin{equation} \label{eq16}
    d_{CL}(G, H) = \max_{\substack{g \in G \\ h \in H}}d(g,h)
\end{equation}

The avarage linkage (AL) is the average between groups:
\begin{equation} \label{eq17}
    d_{AL}(G, H) = \frac{1}{|G| \times |H|}\sum_{g \in G}\sum_{h \in H}d(g,h)
\end{equation}

Finally, in the ward linkage (WL) for each cluster a error function is defined. This error function is the average distance of each datapoint in a cluster to the center of gravity in the cluster:
\begin{equation} \label{eq18}
    d_{WL}(G, H) = \frac{n_Gn_H}{n_G + n_H}||\vec{m}_G - \vec{m}_H||^2
\end{equation}

The one we choose to use is called Ward Linkage. Unlike the others, instead of measuring the distance directly, it analyzes the variance of clusters. Ward’s is said to be the most suitable method for quantitative variables.

Where $\vec{m}_j$ is the center of cluster $j$, and $n_j$ is the number of points in it.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.55\textwidth]{Ward Linkage.png}
    \caption{Ward Linkage \citep{hierarchicaltutorial2017}}
\end{figure}

\subsection{Quality Criteria}

A clustering algorithm aims to gather objects into homogeneous groups that are far one from another. In this section, we focus on those scores to evaluate clusters when ground truth labels are known or unknown.

\subsubsection{Silhouette Coefficient}

If the ground truth labels are not known, evaluation must be performed using the model itself. The Silhouette Coefficient \citep{rousseeuw1987silhouettes} is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores:

\begin{itemize}
    \item \textbf{a:} The mean distance between a sample and all other points in the same class.
    \item \textbf{b:} The mean distance between a sample and all other points in the \textit{next nearest cluster}.
\end{itemize}

The Silhouette Coefficient s for a single sample is then given as:

\begin{equation} \label{eq19}
    s = \frac{b - a}{max(a,b)}
\end{equation}

Silhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.

\subsubsection{Fowlkes-Mallows scores}

The Fowlkes-Mallows index \citep{fowlkes1983method} can be used when the ground truth class assignments of the samples is known. The Fowlkes-Mallows score FMI is defined as the geometric mean of the pairwise precision and recall:

\begin{equation} \label{eq20}
    FMI = \frac{TP}{\sqrt{(TP + FP)(TP + FN)}}
\end{equation}

Where $TP$ is the number of \textbf{True Positive} (i.e. the number of pair of points that belong to the same clusters in both the true labels and the predicted labels), $FP$ is the number of \textbf{False Positive} (i.e. the number of pair of points that belong to the same clusters in the true labels and not in the predicted labels) and $FN$ is the number of \textbf{False Negative} (i.e the number of pair of points that belongs in the same clusters in the predicted labels and not in the true labels).

Since the index is directly proportional to the number of true positives, a higher index means greater similarity between the two clusterings used to determine the index. One basic way to test the validity of this index is to compare two clusterings that are unrelated to each other. \cite{fowlkes1983method} showed that on using two unrelated clusterings, the value of this index approaches zero as the number of total data points chosen for clustering increase; whereas the value for the Rand index \citep{steinley2004properties} for the same data quickly approaches making Fowlkes–Mallows index a much more accurate representation for unrelated data. This index also performs well if noise is added to an existing dataset and their similarity compared. \cite{fowlkes1983method} showed that the value of the index decreases as the component of the noise increases. The index also showed similarity even when the noisy dataset had a different number of clusters than the clusters of the original dataset. Thus making it a reliable tool for measuring similarity between two clusters. 

The score ranges from 0 to 1. A high value indicates a good similarity between two clusters.

\section{Implementation}

In this section, we evaluate and compare 5 distances LCSS, DTW, Hausdorff, SSPD and HCSIM. All these distances have been implemented in Python. 

We also used Python for the implementation of the chosen clustering algorithms, the \textit{scipy} library for \textit{hierarchical clustering analysis}.

\subsection{Ground Truth Data}

\subsubsection{Data}

The data we used are GPS data collected from Mopsi. Mopsi is a website that helps users to find where their friends are and what is around them \citep{mariescu2013detecting}. There are trajectories recorded by users. Those trajectories are displayed on the map with detailed information, such as speed, traveled distance and user’s transportation mode (walking, running, cycling, skiing) which is automatically inferred by the method in \cite{waga2012detecting}. Mopsi allows the user to search trajectories in different ways. It also provides recommendations and tools for managing data collection.

Mopsi data has two types: geo-tagged photos and trajectories. Geo-tagged photos contain information about their location and recorded time. The trajectory is a set of GPS point stored at a fixed interval. In this thesis, we use those trajectories as the data source. There were 444 trajectories in Joensuu, Finland extracted from Mopsi data as shown in \autoref{fig17}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{Mopsi Trajectories.png}
    \caption{Mopsi Trajectories}
    \label{fig17}
\end{figure}

The data structure is in \autoref{table:1}.

\begin{table}[h!]
    \centering
    \def\arraystretch{3}%
    \begin{tabular}{||l l l l||} 
     \hline
     \textbf{Column} & \textbf{Type} & \textbf{Description} & \textbf{Example} \\ [0.5ex] 
     \hline\hline
     Latitude & Double & Point latitude & 62.926880 (62° 55' 36.7674") \\ 
     Longitude & Double & Point longitude & 23.184691 (23° 11' 4.8876") \\
     Timestamp & String & Point timestamp & 1559983789 seconds \\
     Altitude & Double & Point altitude & -1.0 meter \\ [1ex] 
     \hline
    \end{tabular}
    \caption{Data structure}
    \label{table:1}
\end{table}

\subsubsection{Trajectory Similarity}

To cluster “similar” trajectories together, it is essential to formulate
a similarity measure between two trajectories. The process computed trajectory similarity was illustrated as follows:

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{Process.png}
    \caption{Overview of our process}
    \label{fig18}
\end{figure}

\autoref{fig18} shows our clustering process that consists of two components. We first identified clues among trajectories then uses them to measure to measure pair-wise similarity among trajectories. With a set of trajectories with pair-wise similarity measures, we labeled trajectories to capture similarity relationship among trajectory pairs. After that based on the label we are able to obtain clusters of trajectories that have high similarity.

\subsubsection{Analysis of the distances}

Hierarchical Clustering models help to understand trajectory by multiple features, so this treetype construction is proper to implement. As shown in \autoref{fig14}, two hierarchical types are also known as “bottom-up” and “top-down” approaches.

In Agglomerative frameworks, trajectories are grouped and the similar clusters are merged by searching their common properties. Optimal clusters are obtained by repeating representation computation and clusters merging until meeting the requirements. Different from Agglomerative, Divisive frameworks cluster trajectory data into groups and split them recursively to reach the requirements. And as mentioned before, we chose Ward Linkage method on HAC (Hierarchical Agglomerative Clustering).

Below figures are Mopsi clustering results using 5 distances LCSS, DTW, Hausdorff, SSPD and HCSIM with Ward Linkage cluter method.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{lcss_trajectory_clustering.png}
    \caption{LCSS Trajectories Clustering}
    \label{fig19}
\end{figure}

\pagebreak

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{dtw_trajectory_clustering.png}
    \caption{DTW Trajectories Clustering}
    \label{fig20}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{hausdorff_trajectory_clustering.png}
    \caption{Hausdorff Trajectories Clustering}
    \label{fig21}
\end{figure}

\pagebreak

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{sspd_trajectory_clustering.png}
    \caption{SSPD Trajectories Clustering}
    \label{fig22}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{hcsim_trajectory_clustering.png}
    \caption{HCSIM Trajectories Clustering}
    \label{fig23}
\end{figure}

The Warping-based distances, LCSS and DTW, give the poorest results with LCSS being significantly worse than DTW. The shape-based distance Hausdorff and SSPD give better result. These results confirm that shape-based distances are better adapted than warping-based distances.

\subsubsection{Analysis of the number of cluster selection}

We used \textit{Silhouette Score} to decided the number of clusters.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{distance_compare.png}
    \caption{Silhouette Score depending on cluster}
    \label{fig24}
\end{figure}

\autoref{fig24} illustrated Silhouette Score based on number of clusters using 5 distances. 10 clusters are found with LCSS, 13 with DTW, 9 with Hausdorff and 11 with SSPD, HCSIM. We got the groundtruth data, and the right number of clusters is 11. So SSPD and HCSIM give the best results.

As 11 is the number of clusters, so we used Fowlkes Mallows index to evaluate cluster results from those 5 distances.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{fowlkes_mallows_score.png}
    \caption{Fowlkes–Mallows Score based on distance}
    \label{fig25}
\end{figure}

\pagebreak

\autoref{fig25} showed that SSPD and HCSIM (shaped base distance) give the better cluster results than warping-based distances with Fowlkes–Mallows Score is over 0.85.

\subsection{Unknown Data}

\subsubsection{Data}
The data we used are GPS data from 536 San-Francisco taxis over a 24-day period. These data are public and can be found on \cite{piorkowski2009crawdad}. We extracted data as shown in \autoref{fig26}.

This data is a blend of 2574 trajectories. All have the same pickup location, the Caltrain station, and all have a drop-off location in downtown San-Francisco.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{caltrain_trajectory_map.png}
    \caption{Caltrain Trajectories Dataset}
    \label{fig26}
\end{figure}

\subsubsection{Analysis of the distances}

We used Calinski-Harabasz Index \citep{calinski1974dendrite} to evaluate cluster size for all distances.

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{calinski harabasz.png}
    \caption{Calinski-Harabasz Index}
    \label{fig27}
\end{figure}

\pagebreak

The Warping-based distances, LCSS and DTW, give the poorest results with LCSS being significantly worse than DTW. The two shape-based distances Frechet and Haus-dorff give better results.

Below is the visual results for all distances with 15 clusters. Based on Mopsi experiment, SSPD and HCSIM are expected to be the best. We observe that trajectories are well classified by SSPD and HCSIM according to their paths in \ref{fig28}. The clusters using SSPD and HCSIM seem to be consistent. We obtain a partition of traffic based on the taxi drivers’ behavior leaving the Caltrain station in San Francisco.

\pagebreak

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{Caltrain Plots.png}
    \caption{Caltrain Clusters}
    \label{fig28}
\end{figure}

\section{Conclustions}
Clustering is an efﬁcient way to analyze and ﬁnd the massive,hidden, unknown and interesting knowledge in large scale dataset, which facilitates the rapid development of data mining technology in recent decades. With the development of location based service, moving object
clustering becomes a burgeoning topic in related ﬁelds as an essential part of data mining technology. Clustering of non Euclidean objects deeply relies on the choice of a proper distance. In this thesis, we presented different distances focusing on different features of such objects. Based on the experiment, shape-based distances especially SSPD and HCSIM give better clusters.

\pagebreak

% Two common styles, pick one or find alternate bst.
% \citet{john} is like John (1985); \citep{jane} is like (Jane 1985)
%\bibliographystyle{IEEEtranN} % Like this (Jane 1995)
\bibliographystyle{apacite} % Like this [6] 
\bibliography{thesis}

\end{document}